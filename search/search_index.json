{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"video-sampler","text":"<p>Video sampler allows you to efficiently sample video frames and summarize the videos. Currently, it uses keyframe decoding, frame interval gating and perceptual hashing to reduce duplicated samples.</p> <p>Use case: video data collection for machine learning, video summarisation, video frame analysis.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>video-sampler</li> <li>Table of Contents</li> <li>Documentation</li> <li>Features</li> <li>Installation and Usage<ul> <li>Basic usage</li> <li>Image sampling</li> <li>YT-DLP integration plugin<ul> <li>Extra YT-DLP options</li> </ul> </li> <li>OpenAI summary</li> <li>API examples</li> <li>Advanced usage</li> <li>Gating</li> <li>CLIP-based gating comparison</li> <li>Blur gating</li> </ul> </li> <li>Benchmarks</li> <li>Benchmark videos</li> <li>Flit commands<ul> <li>Build</li> <li>Install</li> <li>Publish</li> </ul> </li> <li>\ud83d\udee1 License</li> <li>\ud83d\udcc3 Citation</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<p>Documentation is available at https://lemurpwned.github.io/video-sampler/.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>[x] Direct sampling methods:</li> <li>[x] <code>hash</code> - uses perceptual hashing to reduce duplicated samples</li> <li>[x] <code>entropy</code> - uses entropy to reduce duplicated samples (work in progress)</li> <li>[x] <code>gzip</code> - uses gzip compressed size to reduce duplicated samples (work in progress)</li> <li>[x] <code>buffer</code> - uses sliding buffer to reduce duplicated samples</li> <li>[x] <code>grid</code> - uses grid sampling to reduce duplicated samples</li> <li>[x] Gating methods (modifications on top of direct sampling methods):</li> <li>[x] <code>clip</code> - uses CLIP to filter out frames that do not contain the specified objects</li> <li>[x] <code>blur</code> - uses blur detection to filter out frames that are too blurry</li> <li>[x] Language capture:</li> <li>[x] Keyword capture from subtitles</li> <li>[x] Integrations</li> <li>[x] YTDLP integration -- streams directly from yt-dlp queries,         playlists or single videos</li> <li>[x] OpenAI multimodal models integration for video summaries</li> </ul>"},{"location":"#installation-and-usage","title":"Installation and Usage","text":"<p>If you intend to use all the integrations, you need all the dependencies:</p> <pre><code>python3 -m pip install -U video_sampler[all]\n</code></pre> <p>for minimalist no-cli usage install:</p> <pre><code>python3 -m pip install -U video_sampler\n</code></pre> <p>Available extras are:</p> <ul> <li><code>yt-dlp</code> - for YT-DLP integration</li> <li><code>clip</code> - for CLIP models integration</li> <li><code>language</code> - for language capture</li> <li><code>all</code> - for all dependencies</li> <li><code>dev</code> - for development dependencies</li> </ul> <p>To see all available options, run:</p> <pre><code>python3 -m video_sampler --help\n</code></pre>"},{"location":"#basic-usage","title":"Basic usage","text":"<p>Plain:</p> <pre><code>python3 -m video_sampler hash FatCat.mp4 ./dataset-frames/ --hash-size 3 --buffer-size 20\n</code></pre> <p>From the config file (this is the recommended way if you plan to re-use the same config for different videos):</p> <pre><code>python3 -m video_sampler config ./configs/hash_base.yaml /my-video-folder/ ./my-output-folder\n</code></pre> <p>You can set the number of workers to use with the <code>n_workers</code> parameter. The default is 1.</p>"},{"location":"#image-sampling","title":"Image sampling","text":"<p>If your frames are ordered, then you can use the <code>image_sampler</code> module to sample them. The images should have some concept of ordering, e.g. they should be named in a way that allows for sorting, e.g. <code>image_001.png</code>, <code>image_002.png</code>, etc, because the sampler will deduplicate based on the circular buffer of hashes. An example of a config for <code>image_sampler</code> is given in ./configs/image_base.yaml. Key changes respective to <code>video_sampler</code> are:</p> <ul> <li><code>frame_time_regex</code> - regex to extract frame time from the filename. If not provided, the frames will be lexiographically ordered.</li> <li>any video sampling params such as <code>min_frame_interval_sec</code>, <code>keyframes_only</code> will be disregarded.</li> </ul> <p>You can run the image sampler with -- you need to specify the <code>images</code> flag.</p> <pre><code>python3 -m video_sampler config ./configs/image_base.yaml \"./folder-frames/worlds-smallest-cat-bbc\" ./sampled-output/ --images\n</code></pre>"},{"location":"#yt-dlp-integration-plugin","title":"YT-DLP integration plugin","text":"<p>Before using please consult the ToS of the website you are scraping from -- use responsibly and for research purposes. To use the YT-DLP integration, you need to install <code>yt-dlp</code> first (see yt-dlp). Then, you simply add <code>--yt-dlp</code> to the command, and it changes the meaning of the <code>video_path</code> argument.</p> <ul> <li>to search</li> </ul> <pre><code>video_sampler hash \"ytsearch:cute cats\" ./folder-frames/ \\\n  --hash-size 3 --buffer-size 20 --ytdlp\n</code></pre> <ul> <li>to sample a single video</li> </ul> <pre><code>video_sampler hash \"https://www.youtube.com/watch?v=W86cTIoMv2U\" ./folder-frames/ \\\n    --hash-size 3 --buffer-size 20 --ytdlp\n</code></pre> <ul> <li>to sample a playlist</li> </ul> <pre><code>video_sampler hash \"https://www.youtube.com/watch?v=GbpP3Sxp-1U&amp;list=PLFezMcAw96RGvTTTbdKrqew9seO2ZGRmk\" ./folder-frames/ \\\n  --hash-size 3 --buffer-size 20 --ytdlp\n</code></pre> <ul> <li>segment based on the keyword extraction</li> </ul> <pre><code>video_sampler hash \"https://www.youtube.com/watch?v=GbpP3Sxp-1U&amp;list=PLFezMcAw96RGvTTTbdKrqew9seO2ZGRmk\" ./folder-frames/ \\\n  --hash-size 3 --buffer-size 20 --ytdlp --keywords \"cat,dog,another keyword,test keyword\"\n</code></pre> <p>The videos are never directly downloaded, only streamed, so you can use it to sample videos from the internet without downloading them first.</p>"},{"location":"#extra-yt-dlp-options","title":"Extra YT-DLP options","text":"<p>You can pass extra options to yt-dlp by using the <code>-yt-extra-args</code> flag. For example:</p> <p>this will only sample videos uploaded before 2019-01-01:</p> <pre><code>... --ytdlp --yt-extra-args '--datebefore 20190101'\n</code></pre> <p>or this will only sample videos uploaded after 2019-01-01:</p> <pre><code>... --ytdlp --yt-extra-args '--dateafter 20190101'\n</code></pre> <p>or this will skip all shorts:</p> <pre><code>... --ytdlp --yt-extra-args '--match-filter \"original_url!*=/shorts/ &amp; url!*=/shorts/\"\n</code></pre>"},{"location":"#openai-summary","title":"OpenAI summary","text":"<p>To use the OpenAI multimodal models integration, you need to install <code>openai</code> first <code>pip install openai</code>. Then, you simply add <code>--summary-interval</code> to the command and the url.</p> <p>In the example, I'm using llamafile LLAVA model to summarize the video every 50 frames. If you want to use the OpenAI multimodal models, you need to export <code>OPENAI_API_KEY=your_api_key</code> first. The format should also work with default OpenAI stuff.</p> <p>To replicate, run LLAVA model locally and set the <code>summary-url</code> to the address of the model. Specify the <code>summary-interval</code> to the minimal interval in seconds between frames that are to be summarised/described.</p> <pre><code>video_sampler hash ./videos/FatCat.mp4 ./output-frames/ --hash-size 3 --buffer-size 20 --summary-url \"http://localhost:8080/completion\" --summary-interval 50\n</code></pre> <p>Supported env in case you need those:</p> <ul> <li><code>OPENAI_API_KEY</code> - OpenAI API key</li> <li><code>OPENAI_MODEL</code> - OpenAI model name</li> </ul> <p>Confirmed that you can make it work with e.g. LM Studio, but you need to adjust the <code>summary-url</code> to the correct address, e.g. it might be <code>\"http://localhost:8080/completions\"</code>. Similar if you want to use the OpenAI API.</p> <p>Some frames, based on the interval specified, will be summarised by the model and the result will saved in the <code>./output-frames/summaries.json</code> folder. The frames that are summarised come after the sampling and gating process happens, and only those frames that pass both stages are viable for summarisation.</p> <pre><code>summaries.jsonl\n---\n{\"time\": 56.087, \"summary\": \"A cat is walking through a field of tall grass, with its head down and ears back. The cat appears to be looking for something in the grass, possibly a mouse or another small creature. The field is covered in snow, adding a wintry atmosphere to the scene.\"}\n{\"time\": 110.087, \"summary\": \"A dog is walking in the snow, with its head down, possibly sniffing the ground. The dog is the main focus of the image, and it appears to be a small animal. The snowy landscape is visible in the background, creating a serene and cold atmosphere.\"}\n{\"time\": 171.127, \"summary\": \"The image features a group of animals, including a dog and a cat, standing on a beach near the ocean. The dog is positioned closer to the left side of the image, while the cat is located more towards the center. The scene is set against a beautiful backdrop of a blue sky and a vibrant green ocean. The animals appear to be enjoying their time on the beach, possibly taking a break from their daily activities.\"}\n</code></pre>"},{"location":"#api-examples","title":"API examples","text":"<p>See examples in https://github.com/LemurPwned/video-sampler/tree/main/scripts.</p>"},{"location":"#advanced-usage","title":"Advanced usage","text":"<p>There are 3 sampling methods available:</p> <ul> <li><code>hash</code> - uses perceptual hashing to reduce duplicated samples</li> <li><code>entropy</code> - uses entropy to reduce duplicated samples (work in progress)</li> <li><code>gzip</code> - uses gzip compressed size to reduce duplicated samples (work in progress)</li> </ul> <p>To launch any of them you can run and substitute <code>method-name</code> with one of the above:</p> <pre><code>video_sampler buffer `method-name` ...other options\n</code></pre> <p>e.g.</p> <pre><code>video_sampler buffer entropy --buffer-size 20 ...\n</code></pre> <p>where <code>buffer-size</code> for <code>entropy</code> and <code>gzip</code> mean the top-k sliding buffer size. Sliding buffer also uses hashing to reduce duplicated samples.</p>"},{"location":"#gating","title":"Gating","text":"<p>Aside from basic sampling rules, you can also apply gating rules to the sampled frames, further reducing the number of frames. There are 3 gating methods available:</p> <ul> <li><code>pass</code> - pass all frames</li> <li><code>clip</code> - use CLIP to filter out frames that do not contain the specified objects</li> <li><code>blur</code> - use blur detection to filter out frames that are too blurry</li> </ul> <p>Here's a quick example of how to use clip:</p> <pre><code>python3 -m video_sampler clip ./videos ./scratch/clip --pos-samples \"a cat\" --neg-samples \"empty background, a lemur\"  --hash-size 4\n</code></pre>"},{"location":"#clip-based-gating-comparison","title":"CLIP-based gating comparison","text":"<p>Here's a brief comparison of the frames sampled with and without CLIP-based gating with the following config:</p> <pre><code>  gate_def = dict(\n      type=\"clip\",\n      pos_samples=[\"a cat\"],\n      neg_samples=[\n          \"an empty background\",\n          \"text on screen\",\n          \"a forest with no animals\",\n      ],\n      model_name=\"ViT-B-32\",\n      batch_size=32,\n      pos_margin=0.2,\n      neg_margin=0.3,\n  )\n</code></pre> <p>Evidently, CLIP-based gating is able to filter out frames that do not contain a cat and in consequence, reduce the number of frames with plain background. It also thinks that a lemur is a cat, which is not entirely wrong as fluffy creatures go.</p> Pass gate (no gating) CLIP gate Grid <p>The effects of gating in numbers, for this particular set of examples (see <code>produced</code> vs <code>gated</code> columns). <code>produced</code> represents the number of frames sampled without gating, here after the perceptual hashing, while <code>gated</code> represents the number of frames sampled after gating.</p> video buffer gate decoded produced gated FatCat.mp4 grid pass 179 31 31 SmolCat.mp4 grid pass 118 24 24 HighLemurs.mp4 grid pass 161 35 35 FatCat.mp4 hash pass 179 101 101 SmolCat.mp4 hash pass 118 61 61 HighLemurs.mp4 hash pass 161 126 126 FatCat.mp4 hash clip 179 101 73 SmolCat.mp4 hash clip 118 61 31 HighLemurs.mp4 hash clip 161 126 66"},{"location":"#blur-gating","title":"Blur gating","text":"<p>Helps a little with blurry videos. Adjust threshold and method (<code>laplacian</code> or <code>fft</code>) for best results. Some results from <code>fft</code> at <code>threshold=20</code>:</p> video buffer gate decoded produced gated MadLad.mp4 grid pass 120 31 31 MadLad.mp4 hash pass 120 110 110 MadLad.mp4 hash blur 120 110 85"},{"location":"#benchmarks","title":"Benchmarks","text":"<p>Configuration for this benchmark:</p> <pre><code>SamplerConfig(min_frame_interval_sec=1.0, keyframes_only=True, buffer_size=30, hash_size=X, queue_wait=0.1, debug=True)\n</code></pre> Video Total frames Hash size Decoded Saved SmolCat 2936 8 118 106 SmolCat - 4 - 61 Fat Cat 4462 8 179 163 Fat Cat - 4 - 101 HighLemurs 4020 8 161 154 HighLemurs - 4 - 126 <pre><code>SamplerConfig(\n    min_frame_interval_sec=1.0,\n    keyframes_only=True,\n    queue_wait=0.1,\n    debug=False,\n    print_stats=True,\n    buffer_config={'type': 'entropy'/'gzip', 'size': 30, 'debug': False, 'hash_size': 8, 'expiry': 50}\n)\n</code></pre> Video Total frames Type Decoded Saved SmolCat 2936 entropy 118 39 SmolCat - gzip - 39 Fat Cat 4462 entropy 179 64 Fat Cat - gzip - 73 HighLemurs 4020 entropy 161 59 HighLemurs - gzip - 63"},{"location":"#benchmark-videos","title":"Benchmark videos","text":"<ul> <li>SmolCat</li> <li>Fat Cat</li> <li>HighLemurs</li> <li>MadLad</li> </ul>"},{"location":"#flit-commands","title":"Flit commands","text":""},{"location":"#build","title":"Build","text":"<pre><code>flit build\n</code></pre>"},{"location":"#install","title":"Install","text":"<pre><code>flit install\n</code></pre>"},{"location":"#publish","title":"Publish","text":"<p>Remember to bump the version in <code>pyproject.toml</code> before publishing.</p> <pre><code>flit publish\n</code></pre>"},{"location":"#license","title":"\ud83d\udee1 License","text":"<p>This project is licensed under the terms of the <code>MIT</code> license. See LICENSE for more details.</p>"},{"location":"#citation","title":"\ud83d\udcc3 Citation","text":"<pre><code>@misc{video-sampler,\n  author = {video-sampler},\n  title = {Video sampler allows you to efficiently sample video frames},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/LemurPwned/video-sampler}}\n}\n</code></pre>"},{"location":"cli_help/","title":"CLI Help","text":""},{"location":"cli_help/#main","title":"Main","text":"<pre><code> Usage: python -m video_sampler [OPTIONS] COMMAND [ARGS]...                                                                                                            \n\n Video sampler allows you to efficiently sample video frames from a video file or a list of video files or urls.                                                       \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --install-completion          Install completion for the current shell.                                                                                             \u2502\n\u2502 --show-completion             Show completion for the current shell, to copy it or customize the installation.                                                      \u2502\n\u2502 --help                        Show this message and exit.                                                                                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 buffer   Buffer type can be one of entropy, gzip, hash, passthrough                                                                                                 \u2502\n\u2502 clip     Buffer type can be only of type hash when using CLIP gating.                                                                                               \u2502\n\u2502 config   Create a sampler from a configuration file.                                                                                                                \u2502\n\u2502 hash     Default buffer is the perceptual hash buffer                                                                                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli_help/#config","title":"Config","text":"<pre><code> Usage: python -m video_sampler config [OPTIONS] CONFIG_PATH INPUT_PATH                                                                                                \n                                       OUTPUT_PATH                                                                                                                     \n\n Create a sampler from a configuration file.                                                                                                                           \n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    config_path      TEXT  Path to the configuration file. [default: None] [required]                                                                              \u2502\n\u2502 *    input_path       TEXT  Path to the video file, image folder or a glob pattern. [default: None] [required]                                                      \u2502\n\u2502 *    output_path      TEXT  Path to the output folder. [default: None] [required]                                                                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --ytdlp            --no-ytdlp           Use yt-dlp to download videos from urls. Default is False. Enabling this will treat video_path as an input to ytdlp         \u2502\n\u2502                                         command.                                                                                                                    \u2502\n\u2502                                         [default: no-ytdlp]                                                                                                         \u2502\n\u2502 --yt-extra-args                   TEXT  Extra arguments for YouTube-DLP extraction in classic format. [default: None]                                               \u2502\n\u2502 --images           --no-images          Use image sampler. [default: no-images]                                                                                     \u2502\n\u2502 --help                                  Show this message and exit.                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli_help/#hash","title":"Hash","text":"<pre><code> Usage: python -m video_sampler hash [OPTIONS] VIDEO_PATH OUTPUT_PATH                                                                                                  \n\n Default buffer is the perceptual hash buffer                                                                                                                          \n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    video_path       TEXT  Path to the video file or a glob pattern. [default: None] [required]                                                                    \u2502\n\u2502 *    output_path      TEXT  Path to the output folder. [default: None] [required]                                                                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --min-frame-interval-sec                           FLOAT    Minimum frame interval in seconds. [default: 1.0]                                                       \u2502\n\u2502 --stats                     --no-stats                      Print stats. [default: stats]                                                                           \u2502\n\u2502 --keyframes-only            --no-keyframes-only             Only sample keyframes. [default: keyframes-only]                                                        \u2502\n\u2502 --buffer-size                                      INTEGER  Size of the buffer. [default: 10]                                                                       \u2502\n\u2502 --hash-size                                        INTEGER  Size of the hash. [default: 4]                                                                          \u2502\n\u2502 --queue-wait                                       FLOAT    Time to wait for the queue. [default: 0.1]                                                              \u2502\n\u2502 --start-time-s                                     INTEGER  The starting time for sampling in seconds. [default: 0]                                                 \u2502\n\u2502 --end-time-s                                       INTEGER  The ending time for sampling in seconds. None for no end. [default: None]                               \u2502\n\u2502 --debug                     --no-debug                      Enable debug mode. [default: no-debug]                                                                  \u2502\n\u2502 --threshold                                        FLOAT    Threshold for the blur gate. If 0 then no blur gate is used. [default: 20.0]                            \u2502\n\u2502 --blur-method                                      TEXT     Method to use for blur gate. Can be fft or variance. [default: fft]                                     \u2502\n\u2502 --ytdlp                     --no-ytdlp                      Use yt-dlp to download videos from urls. Default is False. Enabling this will treat video_path as an    \u2502\n\u2502                                                             input to ytdlp command.                                                                                 \u2502\n\u2502                                                             [default: no-ytdlp]                                                                                     \u2502\n\u2502 --yt-extra-args                                    TEXT     Extra arguments for YouTube-DLP extraction in classic format. [default: None]                           \u2502\n\u2502 --keywords                                         TEXT     Comma separated positive keywords for text extraction. [default: None]                                  \u2502\n\u2502 --summary-url                                      TEXT     URL to summarise the video using LLaMA. [default: None]                                                 \u2502\n\u2502 --summary-interval                                 INTEGER  Interval in seconds to summarise the video. [default: -1]                                               \u2502\n\u2502 --n-workers                                        INTEGER  Number of workers to use. Default is 1. Use -1 to use all CPUs. [default: 1]                            \u2502\n\u2502 --help                                                      Show this message and exit.                                                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli_help/#buffer","title":"Buffer","text":"<pre><code> Usage: python -m video_sampler buffer [OPTIONS] BUFFER_TYPE:{entropy|gzip|hash                                                                                        \n                                       |passthrough|grid} VIDEO_PATH                                                                                                   \n                                       OUTPUT_PATH                                                                                                                     \n\n Buffer type can be one of entropy, gzip, hash, passthrough                                                                                                            \n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    buffer_type      BUFFER_TYPE:{entropy|gzip|hash|passthrough|grid}  [default: None] [required]                                                                  \u2502\n\u2502 *    video_path       TEXT                                              Path to the video file or a glob pattern. [default: None] [required]                        \u2502\n\u2502 *    output_path      TEXT                                              Path to the output folder. [default: None] [required]                                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --min-frame-interval-sec                           FLOAT    Minimum frame interval in seconds. [default: 1.0]                                                       \u2502\n\u2502 --stats                     --no-stats                      Print stats. [default: stats]                                                                           \u2502\n\u2502 --keyframes-only            --no-keyframes-only             Only sample keyframes. [default: keyframes-only]                                                        \u2502\n\u2502 --buffer-size                                      INTEGER  Size of the buffer. [default: 10]                                                                       \u2502\n\u2502 --hash-size                                        INTEGER  Size of the hash. [default: 4]                                                                          \u2502\n\u2502 --expiry                                           INTEGER  Expiry time for the buffer. [default: 4]                                                                \u2502\n\u2502 --queue-wait                                       FLOAT    Time to wait for the queue. [default: 0.1]                                                              \u2502\n\u2502 --start-time-s                                     INTEGER  The starting time for sampling in seconds. [default: 0]                                                 \u2502\n\u2502 --end-time-s                                       INTEGER  The ending time for sampling in seconds. None for no end. [default: None]                               \u2502\n\u2502 --debug                     --no-debug                      Enable debug mode. [default: no-debug]                                                                  \u2502\n\u2502 --grid-size                                        INTEGER  Grid size for the grid buffer. [default: 4]                                                             \u2502\n\u2502 --max-hits                                         INTEGER  Max hits for the grid buffer. [default: 2]                                                              \u2502\n\u2502 --threshold                                        FLOAT    Threshold for the blur gate. If 0 then no blur gate is used. [default: 20.0]                            \u2502\n\u2502 --blur-method                                      TEXT     Method to use for blur gate. Can be fft or variance. [default: fft]                                     \u2502\n\u2502 --ytdlp                     --no-ytdlp                      Use yt-dlp to download videos from urls. Default is False. Enabling this will treat video_path as an    \u2502\n\u2502                                                             input to ytdlp command.                                                                                 \u2502\n\u2502                                                             [default: no-ytdlp]                                                                                     \u2502\n\u2502 --yt-extra-args                                    TEXT     Extra arguments for YouTube-DLP extraction in classic format. [default: None]                           \u2502\n\u2502 --n-workers                                        INTEGER  Number of workers to use. Default is 1. Use -1 to use all CPUs. [default: 1]                            \u2502\n\u2502 --help                                                      Show this message and exit.                                                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli_help/#clip","title":"Clip","text":"<pre><code> Usage: python -m video_sampler clip [OPTIONS] VIDEO_PATH OUTPUT_PATH                                                                                                  \n\n Buffer type can be only of type hash when using CLIP gating.                                                                                                          \n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    video_path       TEXT  Path to the video file or a glob pattern. [default: None] [required]                                                                    \u2502\n\u2502 *    output_path      TEXT  Path to the output folder. [default: None] [required]                                                                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --pos-samples                                      TEXT     Comma separated positive samples to use for gating. [default: None]                                     \u2502\n\u2502 --neg-samples                                      TEXT     Comma separated negative samples to use for gating. [default: None]                                     \u2502\n\u2502 --pos-margin                                       FLOAT    Positive margin for gating. [default: 0.2]                                                              \u2502\n\u2502 --neg-margin                                       FLOAT    Negative margin for gating. [default: 0.3]                                                              \u2502\n\u2502 --batch-size                                       INTEGER  Batch size for CLIP. [default: 32]                                                                      \u2502\n\u2502 --model-name                                       TEXT     Model name for CLIP. [default: ViT-B-32]                                                                \u2502\n\u2502 --min-frame-interval-sec                           FLOAT    Minimum frame interval in seconds. [default: 1.0]                                                       \u2502\n\u2502 --stats                     --no-stats                      Print stats. [default: stats]                                                                           \u2502\n\u2502 --keyframes-only            --no-keyframes-only             Only sample keyframes. [default: keyframes-only]                                                        \u2502\n\u2502 --buffer-size                                      INTEGER  Size of the buffer. [default: 10]                                                                       \u2502\n\u2502 --hash-size                                        INTEGER  Size of the hash. [default: 4]                                                                          \u2502\n\u2502 --queue-wait                                       FLOAT    Time to wait for the queue. [default: 0.1]                                                              \u2502\n\u2502 --start-time-s                                     INTEGER  The starting time for sampling in seconds. [default: 0]                                                 \u2502\n\u2502 --end-time-s                                       INTEGER  The ending time for sampling in seconds. None for no end. [default: None]                               \u2502\n\u2502 --debug                     --no-debug                      Enable debug mode. [default: no-debug]                                                                  \u2502\n\u2502 --ytdlp                     --no-ytdlp                      Use yt-dlp to download videos from urls. Default is False. Enabling this will treat video_path as an    \u2502\n\u2502                                                             input to ytdlp command.                                                                                 \u2502\n\u2502                                                             [default: no-ytdlp]                                                                                     \u2502\n\u2502 --yt-extra-args                                    TEXT     Extra arguments for YouTube-DLP extraction in classic format. [default: None]                           \u2502\n\u2502 --n-workers                                        INTEGER  Number of workers to use. Default is 1. Use -1 to use all CPUs. [default: 1]                            \u2502\n\u2502 --help                                                      Show this message and exit.                                                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"reference/video_sampler/buffer/","title":"Video sampler","text":""},{"location":"reference/video_sampler/buffer/#video_sampler.buffer.EntropyByffer","title":"<code>EntropyByffer</code>","text":"<p>             Bases: <code>FrameBuffer</code></p> <p>Measure image entropy as a function of the image usability</p> Source code in <code>video_sampler/buffer.py</code> <pre><code>class EntropyByffer(FrameBuffer):\n    \"\"\"Measure image entropy as a function of the image usability\"\"\"\n\n    def __init__(\n        self, size: int, expiry: int, debug_flag: bool = False, hash_size: int = 8\n    ) -&gt; None:\n        self.sliding_top_k_buffer = SlidingTopKBuffer(\n            size=size, expiry=expiry, debug_flag=debug_flag, hash_size=hash_size\n        )\n\n    def get_buffer_state(self) -&gt; list[str]:\n        return self.sliding_top_k_buffer.get_buffer_state()\n\n    def add(self, item: Image.Image, metadata: dict[str, Any]):\n        return self.sliding_top_k_buffer.add(\n            item, {**metadata, \"index\": -item.entropy()}\n        )\n\n    def final_flush(self) -&gt; Iterable[tuple[Image.Image | None, dict]]:\n        return self.sliding_top_k_buffer.final_flush()\n\n    def clear(self):\n        self.sliding_top_k_buffer.clear()\n</code></pre>"},{"location":"reference/video_sampler/buffer/#video_sampler.buffer.FrameBuffer","title":"<code>FrameBuffer</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>video_sampler/buffer.py</code> <pre><code>class FrameBuffer(ABC):\n    @abstractmethod\n    def add(self, item: Image.Image, metadata: dict[str, Any]) -&gt; None | tuple:\n        pass\n\n    @abstractmethod\n    def final_flush(self) -&gt; Iterable[tuple[Image.Image | None, dict]]:\n        \"\"\"Flush the buffer and return the remaining items\"\"\"\n        pass\n\n    @abstractmethod\n    def get_buffer_state(self) -&gt; list[str]:\n        \"\"\"Return the current state of the buffer\"\"\"\n        pass\n\n    @abstractmethod\n    def clear(self):\n        \"\"\"Clear the buffer\"\"\"\n        pass\n</code></pre>"},{"location":"reference/video_sampler/buffer/#video_sampler.buffer.FrameBuffer.clear","title":"<code>clear()</code>  <code>abstractmethod</code>","text":"<p>Clear the buffer</p> Source code in <code>video_sampler/buffer.py</code> <pre><code>@abstractmethod\ndef clear(self):\n    \"\"\"Clear the buffer\"\"\"\n    pass\n</code></pre>"},{"location":"reference/video_sampler/buffer/#video_sampler.buffer.FrameBuffer.final_flush","title":"<code>final_flush()</code>  <code>abstractmethod</code>","text":"<p>Flush the buffer and return the remaining items</p> Source code in <code>video_sampler/buffer.py</code> <pre><code>@abstractmethod\ndef final_flush(self) -&gt; Iterable[tuple[Image.Image | None, dict]]:\n    \"\"\"Flush the buffer and return the remaining items\"\"\"\n    pass\n</code></pre>"},{"location":"reference/video_sampler/buffer/#video_sampler.buffer.FrameBuffer.get_buffer_state","title":"<code>get_buffer_state()</code>  <code>abstractmethod</code>","text":"<p>Return the current state of the buffer</p> Source code in <code>video_sampler/buffer.py</code> <pre><code>@abstractmethod\ndef get_buffer_state(self) -&gt; list[str]:\n    \"\"\"Return the current state of the buffer\"\"\"\n    pass\n</code></pre>"},{"location":"reference/video_sampler/buffer/#video_sampler.buffer.GridBuffer","title":"<code>GridBuffer</code>","text":"<p>             Bases: <code>HashBuffer</code></p> <p>A class representing a grid-based buffer for images. Splits the image into a grid and stores the hashes of the grid cells in a mosaic buffer.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>The maximum size of the buffer.</p> required <code>debug_flag</code> <code>bool</code> <p>A flag indicating whether debug information should be printed.</p> <code>False</code> <code>hash_size</code> <code>int</code> <p>The size of the hash.</p> <code>4</code> <code>grid_x</code> <code>int</code> <p>The number of grid cells in the x-axis.</p> <code>4</code> <code>grid_y</code> <code>int</code> <p>The number of grid cells in the y-axis.</p> <code>4</code> <code>max_hits</code> <code>int</code> <p>The maximum number of hits allowed for a hash.</p> <code>1</code> <p>Attributes:</p> Name Type Description <code>grid_x</code> <code>int</code> <p>The number of grid cells in the x-axis.</p> <code>grid_y</code> <code>int</code> <p>The number of grid cells in the y-axis.</p> <code>max_hits</code> <code>int</code> <p>The maximum number of hits allowed for a hash.</p> <code>mosaic_buffer</code> <code>dict</code> <p>A dictionary storing the mosaic buffer.</p> <p>Methods:</p> Name Description <code>add</code> <p>Adds an image to the buffer along with its metadata.</p> <code>clear</code> <p>Clears the buffer and the mosaic buffer.</p> <code>update_ttl_buffer</code> <p>Updates the buffer by expiring images that are not in the grid.</p> Source code in <code>video_sampler/buffer.py</code> <pre><code>class GridBuffer(HashBuffer):\n    \"\"\"\n    A class representing a grid-based buffer for images.\n    Splits the image into a grid and stores the hashes of the grid cells in a mosaic buffer.\n\n    Args:\n        size (int): The maximum size of the buffer.\n        debug_flag (bool, optional): A flag indicating whether debug information should be printed.\n        hash_size (int, optional): The size of the hash.\n        grid_x (int, optional): The number of grid cells in the x-axis.\n        grid_y (int, optional): The number of grid cells in the y-axis.\n        max_hits (int, optional): The maximum number of hits allowed for a hash.\n\n    Attributes:\n        grid_x (int): The number of grid cells in the x-axis.\n        grid_y (int): The number of grid cells in the y-axis.\n        max_hits (int): The maximum number of hits allowed for a hash.\n        mosaic_buffer (dict): A dictionary storing the mosaic buffer.\n\n    Methods:\n        add(item, metadata):\n            Adds an image to the buffer along with its metadata.\n        clear():\n            Clears the buffer and the mosaic buffer.\n        update_ttl_buffer():\n            Updates the buffer by expiring images that are not in the grid.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        size: int,\n        debug_flag: bool = False,\n        hash_size: int = 4,\n        grid_x: int = 4,\n        grid_y: int = 4,\n        max_hits: int = 1,\n    ) -&gt; None:\n        super().__init__(size, debug_flag, hash_size)\n        self.grid_x = grid_x\n        self.grid_y = grid_y\n        self.max_hits = max_hits\n        self.mosaic_buffer = {}\n\n    def __get_grid_hash(self, item: Image.Image) -&gt; Iterable[str]:\n        \"\"\"Compute grid hashes for a given image\"\"\"\n        for x in range(self.grid_x):\n            for y in range(self.grid_y):\n                yield str(\n                    phash(\n                        item.crop(\n                            (\n                                x * item.width / self.grid_x,\n                                y * item.height / self.grid_y,\n                                (x + 1) * item.width / self.grid_x,\n                                (y + 1) * item.height / self.grid_y,\n                            )\n                        ),\n                        hash_size=self.hash_size,\n                    )\n                )\n\n    def _check_mosaic(self, mosaic_hash: str):\n        return mosaic_hash in self.mosaic_buffer\n\n    def update_ttl_buffer(self):\n        # expire the images that are not in the grid\n        if len(self.ordered_buffer) &gt; self.max_size:\n            to_return_hash, return_data = self.ordered_buffer.popitem(last=False)\n            if to_return_hash is not None:\n                removal_keys = [\n                    img_hash\n                    for img_hash, mosaic_hash in self.mosaic_buffer.items()\n                    if mosaic_hash == to_return_hash\n                ]\n                for key in removal_keys:\n                    del self.mosaic_buffer[key]\n            return return_data\n        return None\n\n    def add(self, item: Image.Image, metadata: dict[str, Any]):\n        hash_ = str(phash(item, hash_size=self.hash_size))\n        if not self._check_duplicate(hash_):\n            # not automatically rejected, check the mosaic buffer\n            hash_hits = 0\n            hash_sets = []\n            for el_hash_ in self.__get_grid_hash(item):\n                if el_hash_ in self.mosaic_buffer:\n                    hash_hits += 1\n                hash_sets.append(el_hash_)\n\n            if hash_hits &lt; self.max_hits:\n                # add image hash to the ttl counter\n                self.ordered_buffer[hash_] = (item, metadata)\n                # add the image to the mosaic buffer\n                # this also automatically overwrites the deleted hashes\n                for el_hash in hash_sets:\n                    self.mosaic_buffer[el_hash] = hash_\n\n            if self.debug_flag:\n                console.print(\n                    f\"\\tHash hits: {hash_hits}\"\n                    f\"\\tHash sets: {len(hash_sets)}\"\n                    f\"\\tHash buffer: {len(self.get_buffer_state())}\"\n                    f\"\\tMosaic buffer: {len(self.mosaic_buffer)}\"\n                )\n        return self.update_ttl_buffer()\n\n    def clear(self):\n        super().clear()\n        self.mosaic_buffer = {}\n</code></pre>"},{"location":"reference/video_sampler/buffer/#video_sampler.buffer.GridBuffer.__get_grid_hash","title":"<code>__get_grid_hash(item)</code>","text":"<p>Compute grid hashes for a given image</p> Source code in <code>video_sampler/buffer.py</code> <pre><code>def __get_grid_hash(self, item: Image.Image) -&gt; Iterable[str]:\n    \"\"\"Compute grid hashes for a given image\"\"\"\n    for x in range(self.grid_x):\n        for y in range(self.grid_y):\n            yield str(\n                phash(\n                    item.crop(\n                        (\n                            x * item.width / self.grid_x,\n                            y * item.height / self.grid_y,\n                            (x + 1) * item.width / self.grid_x,\n                            (y + 1) * item.height / self.grid_y,\n                        )\n                    ),\n                    hash_size=self.hash_size,\n                )\n            )\n</code></pre>"},{"location":"reference/video_sampler/buffer/#video_sampler.buffer.GzipBuffer","title":"<code>GzipBuffer</code>","text":"<p>             Bases: <code>FrameBuffer</code></p> <p>Measure compression size as a function of the image usability</p> Source code in <code>video_sampler/buffer.py</code> <pre><code>class GzipBuffer(FrameBuffer):\n    \"\"\"Measure compression size as a function of the image usability\"\"\"\n\n    def __init__(\n        self, size: int, expiry: int, debug_flag: bool = False, hash_size: int = 8\n    ) -&gt; None:\n        self.sliding_top_k_buffer = SlidingTopKBuffer(\n            size=size, expiry=expiry, debug_flag=debug_flag, hash_size=hash_size\n        )\n\n    def get_buffer_state(self) -&gt; list[str]:\n        return self.sliding_top_k_buffer.get_buffer_state()\n\n    def add(self, item: Image.Image, metadata: dict[str, Any]):\n        compressed_l = len(gzip.compress(item.tobytes()))\n        return self.sliding_top_k_buffer.add(item, {**metadata, \"index\": -compressed_l})\n\n    def final_flush(self) -&gt; Iterable[tuple[Image.Image | None, dict]]:\n        return self.sliding_top_k_buffer.final_flush()\n\n    def clear(self):\n        self.sliding_top_k_buffer.clear()\n</code></pre>"},{"location":"reference/video_sampler/buffer/#video_sampler.buffer.HashBuffer","title":"<code>HashBuffer</code>","text":"<p>             Bases: <code>FrameBuffer</code></p> <p>A buffer that stores frames with their corresponding metadata and checks for duplicates based on image hashes. Args:     size (int): The maximum size of the buffer.     debug_flag (bool, optional): Flag indicating whether to enable debug mode. Defaults to False.     hash_size (int, optional): The size of the image hash. Defaults to 4.</p> <p>Methods:</p> Name Description <code>get_buffer_state</code> <p>Returns the current state of the buffer as a list of image hashes.</p> <code>add</code> <p>Image.Image, metadata: dict[str, Any]) Adds an item to the buffer along with its metadata.</p> <code>final_flush</code> <p>Yields the stored items and their metadata in the buffer.</p> Private Methods <p>__add(item: Image.Image, hash_: str, metadata: dict)     Adds an item to the buffer with the given hash and metadata.</p> <p>__check_duplicate(hash_: str) -&gt; bool:     Checks if the given hash already exists in the buffer and renews its validity if found.</p> Source code in <code>video_sampler/buffer.py</code> <pre><code>class HashBuffer(FrameBuffer):\n    \"\"\"\n    A buffer that stores frames with their corresponding metadata and\n    checks for duplicates based on image hashes.\n    Args:\n        size (int): The maximum size of the buffer.\n        debug_flag (bool, optional): Flag indicating whether to enable debug mode. Defaults to False.\n        hash_size (int, optional): The size of the image hash. Defaults to 4.\n\n    Methods:\n        get_buffer_state() -&gt; list[str]:\n            Returns the current state of the buffer as a list of image hashes.\n\n        add(item: Image.Image, metadata: dict[str, Any])\n            Adds an item to the buffer along with its metadata.\n\n        final_flush() -&gt; Iterable[tuple[Image.Image | None, dict]]:\n            Yields the stored items and their metadata in the buffer.\n\n        clear()\n            Clears the buffer.\n\n    Private Methods:\n        __add(item: Image.Image, hash_: str, metadata: dict)\n            Adds an item to the buffer with the given hash and metadata.\n\n        __check_duplicate(hash_: str) -&gt; bool:\n            Checks if the given hash already exists in the buffer and renews its validity if found.\n\n    \"\"\"\n\n    def __init__(self, size: int, debug_flag: bool = False, hash_size: int = 4) -&gt; None:\n        self.ordered_buffer = OrderedDict()\n        self.max_size = size\n        self.debug_flag = debug_flag\n        self.hash_size = hash_size\n\n    def get_buffer_state(self) -&gt; list[str]:\n        return list(self.ordered_buffer.keys())\n\n    def add(self, item: Image.Image, metadata: dict[str, Any]):\n        hash_ = str(phash(item, hash_size=self.hash_size))\n        if not self._check_duplicate(hash_):\n            return self.__add(hash_, item, metadata)\n        return None\n\n    def __add(self, hash_: str, item: Image.Image, metadata: dict):\n        self.ordered_buffer[hash_] = (item, metadata)\n        if len(self.ordered_buffer) &gt; self.max_size:\n            return self.ordered_buffer.popitem(last=False)[1]\n        return None\n\n    def _check_duplicate(self, hash_: str) -&gt; bool:\n        if hash_ in self.ordered_buffer:\n            # renew the hash validity\n            if self.debug_flag:\n                console.print(\n                    f\"Renewing {hash_}\",\n                    style=f\"bold {Color.red.value}\",\n                )\n            self.ordered_buffer.move_to_end(hash_)\n            return True\n        return False\n\n    def final_flush(self) -&gt; Iterable[tuple[Image.Image | None, dict]]:\n        yield from self.ordered_buffer.values()\n\n    def clear(self):\n        self.ordered_buffer.clear()\n\n    def __len__(self):\n        return len(self.ordered_buffer)\n\n    def __iter__(self):\n        return iter(self.ordered_buffer)\n\n    def __getitem__(self, key: str):\n        return self.ordered_buffer[key]\n\n    def popitem(self, last: bool = True):\n        return self.ordered_buffer.popitem(last=last)\n</code></pre>"},{"location":"reference/video_sampler/buffer/#video_sampler.buffer.SlidingTopKBuffer","title":"<code>SlidingTopKBuffer</code>","text":"<p>             Bases: <code>FrameBuffer</code></p> <p>A class representing a sliding top-k buffer for frames.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>The maximum size of the buffer.</p> required <code>debug_flag</code> <code>bool</code> <p>A flag indicating whether debug information should be printed.</p> <code>False</code> <code>expiry</code> <code>int</code> <p>The expiry count for frames.</p> <code>30</code> <code>hash_size</code> <code>int</code> <p>The size of the hash.</p> <code>8</code> <p>Attributes:</p> Name Type Description <code>sliding_buffer</code> <code>list</code> <p>The sliding buffer implemented as a min heap.</p> <code>max_size</code> <code>int</code> <p>The maximum size of the buffer.</p> <code>debug_flag</code> <code>bool</code> <p>A flag indicating whether debug information should be printed.</p> <code>expiry_count</code> <code>int</code> <p>The expiry count for frames.</p> <code>hash_size</code> <code>int</code> <p>The size of the hash.</p> <p>Methods:</p> Name Description <code>get_buffer_state</code> <p>Returns the current state of the buffer.</p> <code>add</code> <p>Adds a frame to the buffer along with its metadata.</p> <code>final_flush</code> <p>Performs a final flush of the buffer and yields the remaining frames.</p> <code>clear</code> <p>Clears the buffer.</p> Source code in <code>video_sampler/buffer.py</code> <pre><code>class SlidingTopKBuffer(FrameBuffer):\n    \"\"\"\n    A class representing a sliding top-k buffer for frames.\n\n    Args:\n        size (int): The maximum size of the buffer.\n        debug_flag (bool, optional): A flag indicating whether debug information should be printed.\n        expiry (int, optional): The expiry count for frames.\n        hash_size (int, optional): The size of the hash.\n\n    Attributes:\n        sliding_buffer (list): The sliding buffer implemented as a min heap.\n        max_size (int): The maximum size of the buffer.\n        debug_flag (bool): A flag indicating whether debug information should be printed.\n        expiry_count (int): The expiry count for frames.\n        hash_size (int): The size of the hash.\n\n    Methods:\n        get_buffer_state() -&gt; list[str]:\n            Returns the current state of the buffer.\n        add(item, metadata):\n            Adds a frame to the buffer along with its metadata.\n        final_flush() -&gt; Iterable[tuple[Image.Image | None, dict]]:\n            Performs a final flush of the buffer and yields the remaining frames.\n        clear():\n            Clears the buffer.\n\n    \"\"\"\n\n    def __init__(\n        self, size: int, debug_flag: bool = False, expiry: int = 30, hash_size: int = 8\n    ) -&gt; None:\n        # it's a min heap with a fixed size\n        self.sliding_buffer = []\n        self.max_size = size\n        self.debug_flag = debug_flag\n        self.expiry_count = expiry\n        self.hash_size = hash_size\n        assert (\n            self.expiry_count &gt; self.max_size\n        ), \"expiry count must be greater than max size\"\n        console.print(\n            f\"Creating sliding buffer of size {self.max_size} and expiry {expiry}\",\n            style=f\"bold {Color.red.value}\",\n        )\n\n    def get_buffer_state(self) -&gt; list[str]:\n        return [item[:3] for item in self.sliding_buffer]\n\n    def add(self, item: Image.Image, metadata: dict[str, Any]):\n        assert \"index\" in metadata, \"metadata must have index key for sliding buffer\"\n        average_hash_ = str(average_hash(item, hash_size=self.hash_size))\n        to_return = None\n        if not self.__check_duplicate(average_hash_):\n            heapq.heappush(\n                self.sliding_buffer,\n                [metadata[\"index\"], 0, average_hash_, item, metadata],\n            )\n            if len(self.sliding_buffer) &gt;= self.max_size:\n                to_return = heapq.heappop(self.sliding_buffer)[-2:]\n        # update the expiry count\n        expired_indx = -1\n        for i in range(len(self.sliding_buffer)):\n            self.sliding_buffer[i][1] += 1\n            if self.sliding_buffer[i][1] &gt;= self.expiry_count:\n                expired_indx = i\n        # at any point only one item can be expired\n        if expired_indx != -1:\n            self.sliding_buffer.pop(expired_indx)  # just drop\n        return to_return\n\n    def __check_duplicate(self, hash_: str) -&gt; bool:\n        for item in self.sliding_buffer:\n            if item[2] == hash_:\n                # renew the hash validity\n                if self.debug_flag:\n                    console.print(\n                        f\"Renewing {hash_}\",\n                        style=f\"bold {Color.red.value}\",\n                    )\n                item[1] = 0\n                return True\n        return False\n\n    def final_flush(self) -&gt; Iterable[tuple[Image.Image | None, dict]]:\n        if len(self.sliding_buffer):\n            yield heapq.heappop(self.sliding_buffer)[-2:]\n        yield None, {}\n\n    def clear(self):\n        self.sliding_buffer.clear()\n</code></pre>"},{"location":"reference/video_sampler/buffer/#video_sampler.buffer.create_buffer","title":"<code>create_buffer(buffer_config)</code>","text":"<p>Create a buffer based on the config</p> Source code in <code>video_sampler/buffer.py</code> <pre><code>def create_buffer(buffer_config: dict[str, Any]) -&gt; FrameBuffer:\n    \"\"\"Create a buffer based on the config\"\"\"\n    check_buffer_config(buffer_config)\n    console.print(\n        f\"Creating buffer of type {buffer_config['type']}\",\n        style=f\"bold {Color.red.value}\",\n    )\n    if buffer_config[\"type\"] == \"hash\":\n        return HashBuffer(\n            size=buffer_config[\"size\"],\n            debug_flag=buffer_config.get(\"debug\", False),\n            hash_size=buffer_config[\"hash_size\"],\n        )\n    elif buffer_config[\"type\"] == \"grid\":\n        return GridBuffer(\n            size=buffer_config[\"size\"],\n            debug_flag=buffer_config.get(\"debug\", False),\n            hash_size=buffer_config[\"hash_size\"],\n            grid_x=buffer_config[\"grid_x\"],\n            grid_y=buffer_config[\"grid_y\"],\n            max_hits=buffer_config[\"max_hits\"],\n        )\n    elif buffer_config[\"type\"] == \"sliding_top_k\":\n        return SlidingTopKBuffer(\n            size=buffer_config[\"size\"],\n            debug_flag=buffer_config.get(\"debug\", False),\n            expiry=buffer_config[\"expiry\"],\n        )\n    elif buffer_config[\"type\"] == \"passthrough\":\n        return PassThroughBuffer()\n    elif buffer_config[\"type\"] == \"gzip\":\n        return GzipBuffer(\n            size=buffer_config[\"size\"],\n            debug_flag=buffer_config.get(\"debug\", False),\n            hash_size=buffer_config[\"hash_size\"],\n            expiry=buffer_config[\"expiry\"],\n        )\n    elif buffer_config[\"type\"] == \"entropy\":\n        return EntropyByffer(\n            size=buffer_config[\"size\"],\n            debug_flag=buffer_config.get(\"debug\", False),\n            hash_size=buffer_config[\"hash_size\"],\n            expiry=buffer_config[\"expiry\"],\n        )\n    else:\n        raise ValueError(f\"Unknown buffer type {buffer_config['type']}\")\n</code></pre>"},{"location":"reference/video_sampler/config/","title":"Config","text":""},{"location":"reference/video_sampler/config/#video_sampler.config.ImageSamplerConfig","title":"<code>ImageSamplerConfig</code>","text":"<p>             Bases: <code>SamplerConfig</code></p> <p>Configuration options for the image sampler.</p> Source code in <code>video_sampler/config.py</code> <pre><code>class ImageSamplerConfig(SamplerConfig):\n    \"\"\"\n    Configuration options for the image sampler.\n    \"\"\"\n\n    frame_time_regex: str = Field(default=\"\")\n</code></pre>"},{"location":"reference/video_sampler/config/#video_sampler.config.SamplerConfig","title":"<code>SamplerConfig</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Configuration options for the video sampler.</p> <p>Parameters:</p> Name Type Description Default <code>min_frame_interval_sec</code> <code>float</code> <p>The minimum time interval between sampled frames in seconds. Defaults to 1.</p> required <code>keyframes_only</code> <code>bool</code> <p>Flag indicating whether to sample only keyframes. Defaults to True.</p> required <code>queue_wait</code> <code>float</code> <p>The time to wait between checking the frame queue in seconds. Defaults to 0.1.</p> required <code>start_time_s</code> <code>int</code> <p>The starting time for sampling in seconds. Defaults to 0.</p> required <code>end_time_s</code> <code>int</code> <p>The ending time for sampling in seconds. None for no end. Defaults to None.</p> required <code>precise_seek</code> <code>bool</code> <p>Flag indicating whether to use precise seeking. Defaults to False. Precise seeking is slower but more accurate.</p> required <code>debug</code> <code>bool</code> <p>Flag indicating whether to enable debug mode. Defaults to False.</p> required <code>print_stats</code> <code>bool</code> <p>Flag indicating whether to print sampling statistics. Defaults to False.</p> required <code>buffer_config</code> <code>dict[str, Any]</code> <p>Configuration options for     the frame buffer. Defaults to {\"type\": \"entropy\", \"size\": 15,     \"debug\": True}.</p> required <code>gate_config</code> <code>dict[str, Any]</code> <p>Configuration options for     the frame gate. Defaults to {\"type\": \"pass\"}.</p> required <code>extractor_config</code> <code>dict[str, Any]</code> <p>Configuration options for     the extractor (keyword, audio). Defaults to None.</p> required <code>summary_config</code> <code>dict[str, Any]</code> <p>Configuration options for     the summary generator. Defaults to None.</p> required <p>Methods:     str() -&gt; str:         Returns a string representation of the configuration.</p> Source code in <code>video_sampler/config.py</code> <pre><code>class SamplerConfig(BaseModel):\n    \"\"\"\n    Configuration options for the video sampler.\n\n    Args:\n        min_frame_interval_sec (float, optional): The minimum time interval\n            between sampled frames in seconds. Defaults to 1.\n        keyframes_only (bool, optional): Flag indicating whether to\n            sample only keyframes. Defaults to True.\n        queue_wait (float, optional): The time to wait between checking\n            the frame queue in seconds. Defaults to 0.1.\n        start_time_s (int, optional): The starting time for sampling in seconds.\n            Defaults to 0.\n        end_time_s (int, optional): The ending time for sampling in seconds.\n            None for no end. Defaults to None.\n        precise_seek (bool, optional): Flag indicating whether to use precise\n            seeking. Defaults to False. Precise seeking is slower but more\n            accurate.\n        debug (bool, optional): Flag indicating whether to enable debug mode.\n            Defaults to False.\n        print_stats (bool, optional): Flag indicating whether to print\n            sampling statistics. Defaults to False.\n        buffer_config (dict[str, Any], optional): Configuration options for\n                the frame buffer. Defaults to {\"type\": \"entropy\", \"size\": 15,\n                \"debug\": True}.\n        gate_config (dict[str, Any], optional): Configuration options for\n                the frame gate. Defaults to {\"type\": \"pass\"}.\n        extractor_config (dict[str, Any], optional): Configuration options for\n                the extractor (keyword, audio). Defaults to None.\n        summary_config (dict[str, Any], optional): Configuration options for\n                the summary generator. Defaults to None.\n    Methods:\n        __str__() -&gt; str:\n            Returns a string representation of the configuration.\n\n    \"\"\"\n\n    min_frame_interval_sec: float = Field(default=1, ge=0)\n    keyframes_only: bool = Field(default=True)\n    queue_wait: float = Field(default=0.1, ge=1e-3)\n    start_time_s: int = Field(\n        default=0, ge=0, description=\"The starting time for sampling in seconds.\"\n    )\n    end_time_s: int | None = Field(\n        default=None,\n        ge=1,\n        description=\"The ending time for sampling in seconds. None for no end.\",\n    )\n    precise_seek: bool = Field(default=False, description=\"Use precise seeking.\")\n    debug: bool = Field(default=False)\n    print_stats: bool = Field(default=False)\n    buffer_config: dict[str, Any] = Field(\n        default_factory=lambda: {\n            \"type\": \"hash\",\n            \"hash_size\": 8,\n            \"size\": 15,\n            \"debug\": True,\n        }\n    )\n    gate_config: dict[str, Any] = Field(\n        default_factory=lambda: {\n            \"type\": \"pass\",\n        }\n    )\n    extractor_config: dict[str, Any] = Field(default_factory=dict)\n    summary_config: dict[str, Any] = Field(default_factory=dict)\n    n_workers: int = Field(default=1)\n\n    save_format: SaveFormatConfig = Field(default_factory=SaveFormatConfig)\n\n    def __str__(self) -&gt; str:\n        return str(self.model_dump())\n\n    @classmethod\n    def from_yaml(cls, file_path: str) -&gt; \"SamplerConfig\":\n        with open(file_path) as file:\n            data = yaml.safe_load(file)\n        return cls(**data)\n\n    @model_validator(mode=\"after\")\n    def validate_start_end_times(cls, values: \"SamplerConfig\"):\n        if values.end_time_s is not None and values.start_time_s &gt;= values.end_time_s:\n            raise ValueError(\"start_time_s must be strictly less than the end_time_s\")\n        return values\n</code></pre>"},{"location":"reference/video_sampler/config/#video_sampler.config.SaveFormatConfig","title":"<code>SaveFormatConfig</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Configuration options for the save format.</p> <p>Parameters:</p> Name Type Description Default <code>encode_time_b64</code> <code>bool</code> <p>Encode time as base64 string. Avoids <code>.</code> in the filename. Defaults to False.</p> required <code>avoid_dot</code> <code>bool</code> <p>Avoid <code>.</code> in the filename by replacing it with <code>_</code>. Defaults to False.</p> required <code>include_filename</code> <code>bool</code> <p>Include filename in the output path. Defaults to False.</p> required Source code in <code>video_sampler/config.py</code> <pre><code>class SaveFormatConfig(BaseModel):\n    \"\"\"\n    Configuration options for the save format.\n\n    Args:\n        encode_time_b64 (bool, optional): Encode time as base64 string. Avoids `.`\n            in the filename. Defaults to False.\n        avoid_dot (bool, optional): Avoid `.` in the filename by replacing\n            it with `_`. Defaults to False.\n        include_filename (bool, optional): Include filename in the output\n            path. Defaults to False.\n    \"\"\"\n\n    encode_time_b64: bool = Field(default=False)\n    avoid_dot: bool = Field(default=False)\n    include_filename: bool = Field(default=False)\n\n    @model_validator(mode=\"after\")\n    def validate_save_format(cls, values: \"SaveFormatConfig\"):\n        if values.encode_time_b64 and values.avoid_dot:\n            import warnings\n\n            warnings.warn(\n                \"encode_time_b64 and avoid_dot are both True, \"\n                \"avoid_dot will be ignored\",\n                UserWarning,\n                stacklevel=2,\n            )\n        return values\n</code></pre>"},{"location":"reference/video_sampler/evaluation/","title":"Evaluation","text":""},{"location":"reference/video_sampler/evaluation/#video_sampler.evaluation.compute_total_video_entropy","title":"<code>compute_total_video_entropy()</code>","text":"<p>Compute the total entropy of a video</p> Source code in <code>video_sampler/evaluation.py</code> <pre><code>def compute_total_video_entropy():\n    \"\"\"Compute the total entropy of a video\"\"\"\n    pass\n</code></pre>"},{"location":"reference/video_sampler/gating/","title":"Gating","text":""},{"location":"reference/video_sampler/gating/#video_sampler.gating.BlurGate","title":"<code>BlurGate</code>","text":"Source code in <code>video_sampler/gating.py</code> <pre><code>class BlurGate:\n    def __init__(\n        self, method: Literal[\"fft\", \"laplacian\"] = \"laplacian\", threshold: float = 100\n    ) -&gt; None:\n        \"\"\"\n        Initializes the Gating object.\n\n        Args:\n            method (str): The method to use for blur detection. Can be \"fft\" or \"laplacian\".\n            threshold (float): The threshold for bluriness. The higher the threshold, the less\n                blurry the image needs to be to be discarded.\n                The default threshold values are:\n                - 20 for the \"fft\" method\n                - 100 for the \"laplacian\" method.\n\n        Raises:\n            ValueError: If an unknown blur method is provided.\n        \"\"\"\n        self.is_blurry = None\n        if method == \"fft\":\n            self.is_blurry = self._is_blurry_fft\n        elif method == \"laplacian\":\n            self.is_blurry = self._is_blurry_laplacian\n        else:\n            raise ValueError(f\"Unknown blur method {method}\")\n        self.threshold = threshold\n\n    def __call__(self, frame: Image.Image, meta: dict, last=False) -&gt; GatedObject:\n        if self.is_blurry(frame) or last:\n            return EMPTY_GATED_OBJECT\n        return GatedObject([FrameObject(frame, meta)], 1)\n\n    def _is_blurry_laplacian(self, frame: Image.Image) -&gt; bool:\n        \"\"\"Check if the image is blurry with laplacian method.\"\"\"\n        return (\n            cv2.Laplacian(\n                cv2.cvtColor(np.array(frame), cv2.COLOR_BGR2GRAY), cv2.CV_64F\n            ).var()\n            &lt; self.threshold\n        )\n\n    def _is_blurry_fft(self, frame: Image.Image) -&gt; bool:\n        \"\"\"Check if the image is blurry with fft method.\"\"\"\n        f = np.fft.fft2(frame)\n        fshift = np.fft.fftshift(f)\n        magnitude_spectrum = 20 * np.log(np.abs(fshift) + 1e-12)\n        return magnitude_spectrum.mean() &lt; self.threshold\n\n    def flush(self):\n        return EMPTY_GATED_OBJECT\n</code></pre>"},{"location":"reference/video_sampler/gating/#video_sampler.gating.BlurGate.__init__","title":"<code>__init__(method='laplacian', threshold=100)</code>","text":"<p>Initializes the Gating object.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method to use for blur detection. Can be \"fft\" or \"laplacian\".</p> <code>'laplacian'</code> <code>threshold</code> <code>float</code> <p>The threshold for bluriness. The higher the threshold, the less blurry the image needs to be to be discarded. The default threshold values are: - 20 for the \"fft\" method - 100 for the \"laplacian\" method.</p> <code>100</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown blur method is provided.</p> Source code in <code>video_sampler/gating.py</code> <pre><code>def __init__(\n    self, method: Literal[\"fft\", \"laplacian\"] = \"laplacian\", threshold: float = 100\n) -&gt; None:\n    \"\"\"\n    Initializes the Gating object.\n\n    Args:\n        method (str): The method to use for blur detection. Can be \"fft\" or \"laplacian\".\n        threshold (float): The threshold for bluriness. The higher the threshold, the less\n            blurry the image needs to be to be discarded.\n            The default threshold values are:\n            - 20 for the \"fft\" method\n            - 100 for the \"laplacian\" method.\n\n    Raises:\n        ValueError: If an unknown blur method is provided.\n    \"\"\"\n    self.is_blurry = None\n    if method == \"fft\":\n        self.is_blurry = self._is_blurry_fft\n    elif method == \"laplacian\":\n        self.is_blurry = self._is_blurry_laplacian\n    else:\n        raise ValueError(f\"Unknown blur method {method}\")\n    self.threshold = threshold\n</code></pre>"},{"location":"reference/video_sampler/gating/#video_sampler.gating.ClipGate","title":"<code>ClipGate</code>","text":"Source code in <code>video_sampler/gating.py</code> <pre><code>class ClipGate:\n    def __init__(\n        self,\n        pos_samples: list[str] = None,\n        neg_samples: list[str] = None,\n        model_name: str = \"ViT-B-32\",\n        batch_size: int = 32,\n        pos_margin: float = 0.2,\n        neg_margin: float = 0.3,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the Clip Gating object.\n\n        Args:\n            pos_samples (list[str], optional): List of positive samples. Defaults to None.\n            neg_samples (list[str], optional): List of negative samples. Defaults to None.\n            model_name (str, optional): Name of the model. Defaults to \"ViT-B-32\".\n            batch_size (int, optional): Batch size. Defaults to 32.\n            pos_margin (float, optional): Positive margin. Defaults to 0.2.\n            neg_margin (float, optional): Negative margin. Defaults to 0.3.\n        \"\"\"\n        self.model, self.preprocess, self.tokenizer = create_model(\n            model_name=model_name\n        )\n        self.pos_margin = pos_margin\n        self.neg_margin = neg_margin\n        self.batch_size = batch_size\n        self.frame_accumulator = []\n        self.metadata_accumulator = []\n        if pos_samples is None:\n            self.pos_samples = torch.zeros((1, 512))\n        else:\n            self.pos_samples = self._preproc_samples(pos_samples)\n        if neg_samples is None:\n            self.neg_samples = torch.zeros((1, 512))\n        else:\n            self.neg_samples = self._preproc_samples(neg_samples)\n\n    def __call__(self, frame: Image.Image, meta: dict, last=False) -&gt; Any:\n        return self.flush() if last else self.add_frame(frame, meta)\n\n    def _preproc_samples(self, sample_texts: list[str]):\n        inputs = self.tokenizer(sample_texts)\n        embeds = torch.zeros((len(sample_texts), 512))\n        with torch.no_grad():\n            for i, batch in enumerate(batched(inputs, n=self.batch_size)):\n                batch = torch.stack(batch)\n                text_embeds = self.model.encode_text(batch.to(DEVICE))\n                embeds[i * self.batch_size : (i + 1) * self.batch_size] = (\n                    text_embeds.cpu()\n                )\n        embeds /= embeds.norm(dim=-1, keepdim=True)\n        return embeds\n\n    def _embed_frames(self, frames: list[Image.Image]):\n        \"\"\"Compute the embeddings for each frame.\"\"\"\n        inputs = torch.stack([self.preprocess(frame) for frame in frames]).to(DEVICE)\n        with torch.no_grad():\n            image_embeds = self.model.encode_image(inputs).cpu()\n            image_embeds /= image_embeds.norm(dim=-1, keepdim=True)\n        return image_embeds\n\n    def _get_margins(self, frame_embeds: \"torch.Tensor\"):\n        \"\"\"Compute the margins for each frame.\"\"\"\n        org_indx = np.arange(frame_embeds.shape[0])\n        neg_distance = frame_embeds @ self.neg_samples.T\n        pos_distance = frame_embeds @ self.pos_samples.T\n        neg_margin, _ = neg_distance.max(axis=-1)\n        pos_margin, _ = pos_distance.max(axis=-1)\n        incl_samples = torch.argwhere(\n            (neg_margin &lt; self.neg_margin) &amp; (pos_margin &gt;= self.pos_margin)\n        )\n        return org_indx[incl_samples].ravel()\n\n    def add_frame(self, frame: Image.Image, metadata: dict) -&gt; GatedObject:\n        self.frame_accumulator.append(frame)\n        self.metadata_accumulator.append(metadata)\n        if len(self.frame_accumulator) == self.batch_size:\n            return self.__process_metadata()\n        return EMPTY_GATED_OBJECT\n\n    def flush(self):\n        return self.__process_metadata()\n\n    def __process_metadata(self) -&gt; GatedObject:\n        frame_embeds = self._embed_frames(self.frame_accumulator)\n        selected_frames = self._get_margins(frame_embeds)\n        to_return = [\n            FrameObject(self.frame_accumulator[i], self.metadata_accumulator[i])\n            for i in range(len(self.frame_accumulator))\n            if i in selected_frames\n        ]\n        self.frame_accumulator.clear()\n        self.metadata_accumulator.clear()\n        return GatedObject(to_return, len(selected_frames))\n</code></pre>"},{"location":"reference/video_sampler/gating/#video_sampler.gating.ClipGate.__init__","title":"<code>__init__(pos_samples=None, neg_samples=None, model_name='ViT-B-32', batch_size=32, pos_margin=0.2, neg_margin=0.3)</code>","text":"<p>Initializes the Clip Gating object.</p> <p>Parameters:</p> Name Type Description Default <code>pos_samples</code> <code>list[str]</code> <p>List of positive samples. Defaults to None.</p> <code>None</code> <code>neg_samples</code> <code>list[str]</code> <p>List of negative samples. Defaults to None.</p> <code>None</code> <code>model_name</code> <code>str</code> <p>Name of the model. Defaults to \"ViT-B-32\".</p> <code>'ViT-B-32'</code> <code>batch_size</code> <code>int</code> <p>Batch size. Defaults to 32.</p> <code>32</code> <code>pos_margin</code> <code>float</code> <p>Positive margin. Defaults to 0.2.</p> <code>0.2</code> <code>neg_margin</code> <code>float</code> <p>Negative margin. Defaults to 0.3.</p> <code>0.3</code> Source code in <code>video_sampler/gating.py</code> <pre><code>def __init__(\n    self,\n    pos_samples: list[str] = None,\n    neg_samples: list[str] = None,\n    model_name: str = \"ViT-B-32\",\n    batch_size: int = 32,\n    pos_margin: float = 0.2,\n    neg_margin: float = 0.3,\n) -&gt; None:\n    \"\"\"\n    Initializes the Clip Gating object.\n\n    Args:\n        pos_samples (list[str], optional): List of positive samples. Defaults to None.\n        neg_samples (list[str], optional): List of negative samples. Defaults to None.\n        model_name (str, optional): Name of the model. Defaults to \"ViT-B-32\".\n        batch_size (int, optional): Batch size. Defaults to 32.\n        pos_margin (float, optional): Positive margin. Defaults to 0.2.\n        neg_margin (float, optional): Negative margin. Defaults to 0.3.\n    \"\"\"\n    self.model, self.preprocess, self.tokenizer = create_model(\n        model_name=model_name\n    )\n    self.pos_margin = pos_margin\n    self.neg_margin = neg_margin\n    self.batch_size = batch_size\n    self.frame_accumulator = []\n    self.metadata_accumulator = []\n    if pos_samples is None:\n        self.pos_samples = torch.zeros((1, 512))\n    else:\n        self.pos_samples = self._preproc_samples(pos_samples)\n    if neg_samples is None:\n        self.neg_samples = torch.zeros((1, 512))\n    else:\n        self.neg_samples = self._preproc_samples(neg_samples)\n</code></pre>"},{"location":"reference/video_sampler/gating/#video_sampler.gating.PassGate","title":"<code>PassGate</code>","text":"Source code in <code>video_sampler/gating.py</code> <pre><code>class PassGate:\n    def __call__(self, frame: Image.Image, meta: dict, last=False) -&gt; GatedObject:\n        \"\"\"\n        Passes the frame through the gating mechanism.\n\n        Args:\n            frame (Image.Image): The frame to pass through.\n            meta (dict): The metadata for the frame.\n            last (bool): If this is the last frame in the video.\n\n        Returns:\n            GatedObject: The gated object containing the processed frame.\n        \"\"\"\n        return self.flush() if last else GatedObject([FrameObject(frame, meta)], 1)\n\n    def flush(self):\n        return EMPTY_GATED_OBJECT\n</code></pre>"},{"location":"reference/video_sampler/gating/#video_sampler.gating.PassGate.__call__","title":"<code>__call__(frame, meta, last=False)</code>","text":"<p>Passes the frame through the gating mechanism.</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>Image</code> <p>The frame to pass through.</p> required <code>meta</code> <code>dict</code> <p>The metadata for the frame.</p> required <code>last</code> <code>bool</code> <p>If this is the last frame in the video.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>GatedObject</code> <code>GatedObject</code> <p>The gated object containing the processed frame.</p> Source code in <code>video_sampler/gating.py</code> <pre><code>def __call__(self, frame: Image.Image, meta: dict, last=False) -&gt; GatedObject:\n    \"\"\"\n    Passes the frame through the gating mechanism.\n\n    Args:\n        frame (Image.Image): The frame to pass through.\n        meta (dict): The metadata for the frame.\n        last (bool): If this is the last frame in the video.\n\n    Returns:\n        GatedObject: The gated object containing the processed frame.\n    \"\"\"\n    return self.flush() if last else GatedObject([FrameObject(frame, meta)], 1)\n</code></pre>"},{"location":"reference/video_sampler/gating/#video_sampler.gating.create_gate","title":"<code>create_gate(gate_config)</code>","text":"<p>Create a gate from a configuration.</p> Source code in <code>video_sampler/gating.py</code> <pre><code>def create_gate(gate_config: dict) -&gt; BlurGate | ClipGate | PassGate:\n    \"\"\"Create a gate from a configuration.\"\"\"\n    gate_type = gate_config[\"type\"]\n    del gate_config[\"type\"]\n    if gate_type == \"pass\":\n        return PassGate()\n    elif gate_type == \"clip\":\n        return ClipGate(**gate_config)\n    elif gate_type == \"blur\":\n        return BlurGate(**gate_config)\n    else:\n        raise ValueError(f\"Unknown gate type {gate_type}\")\n</code></pre>"},{"location":"reference/video_sampler/image_summary/","title":"Image summary","text":""},{"location":"reference/video_sampler/iterators/","title":"Iterators","text":""},{"location":"reference/video_sampler/iterators/#video_sampler.iterators.delegate_workers","title":"<code>delegate_workers(video_path, output_path, cfg, sampler_cls=VideoSampler)</code>","text":"<p>Delegate the processing of a list of videos to a worker instance.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str | Generator</code> <p>Path to a video file, a generator of URLs or a list of video files.</p> required <code>output_path</code> <code>str</code> <p>Path to the output folder.</p> required <code>cfg</code> <code>SamplerConfig</code> <p>Configuration for the worker.</p> required Source code in <code>video_sampler/iterators.py</code> <pre><code>def delegate_workers(\n    video_path: str | Generator,\n    output_path: str,\n    cfg: SamplerConfig | ImageSamplerConfig,\n    sampler_cls: BaseSampler | None = VideoSampler,\n):\n    \"\"\"Delegate the processing of a list of videos to a worker instance.\n\n    Args:\n        video_path (str | Generator): Path to a video file, a generator of URLs or a list of video files.\n        output_path (str): Path to the output folder.\n        cfg (SamplerConfig): Configuration for the worker.\n    \"\"\"\n    msg = \"Detected input as a file\"\n    is_url = False\n    if isinstance(video_path, Generator):\n        videos = video_path\n        msg = \"Detected input as an URL generator\"\n        is_url = True\n    elif not os.path.isfile(video_path) and not isinstance(cfg, ImageSamplerConfig):\n        if \"*\" not in video_path:\n            videos = glob.glob(os.path.join(video_path, \"*\"))\n        else:\n            videos = glob.glob(video_path)\n        msg = f\"Detected input as a folder with {len(videos)} files\"\n    else:\n        videos = iter([video_path])\n    console.print(msg, style=f\"bold {Color.cyan.value}\")\n    if sampler_cls is None:\n        warnings.warn(\n            \"Sampler class was not specified, defaulting to Video Sampler\", stacklevel=2\n        )\n        sampler_cls = VideoSampler\n    parallel_video_processing(\n        videos,\n        output_path,\n        is_url=is_url,\n        n_workers=cfg.n_workers,\n        sampler_cls=sampler_cls,\n        worker_cfg=cfg,\n    )\n    console.print(\"All videos processed\", style=f\"bold {Color.green.value}\")\n</code></pre>"},{"location":"reference/video_sampler/iterators/#video_sampler.iterators.parallel_video_processing","title":"<code>parallel_video_processing(video_iterable, output_path, is_url, worker_cfg, sampler_cls=VideoSampler, n_workers=None)</code>","text":"<p>Process a list of local video files or video URLs in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>video_iterable</code> <code>Iterable[Union[str, tuple]]</code> <p>An iterable of video file paths or video URLs.</p> required <code>output_path</code> <code>str</code> <p>Path to the output folder.</p> required <code>is_url</code> <code>bool</code> <p>Flag to indicate if the video is a URL.</p> required <code>worker_cfg</code> <code>SamplerConfig</code> <p>Configuration for the worker.</p> required <code>n_workers</code> <code>int</code> <p>Number of workers to use.</p> <code>None</code> Source code in <code>video_sampler/iterators.py</code> <pre><code>def parallel_video_processing(\n    video_iterable: Iterable[str | tuple],\n    output_path: str,\n    is_url: bool,\n    worker_cfg: SamplerConfig | ImageSamplerConfig,\n    sampler_cls: BaseSampler | None = VideoSampler,\n    n_workers: int = None,\n):  # sourcery skip: for-append-to-extend\n    \"\"\"Process a list of local video files or video URLs in parallel.\n\n    Args:\n        video_iterable (Iterable[Union[str, tuple]]): An iterable of video file paths or video URLs.\n        output_path (str): Path to the output folder.\n        is_url (bool): Flag to indicate if the video is a URL.\n        worker_cfg (SamplerConfig): Configuration for the worker.\n        n_workers (int): Number of workers to use.\n    \"\"\"\n    if n_workers == -1:\n        n_workers = None\n    if n_workers is not None and n_workers == 1:\n        for video in tqdm(video_iterable, desc=\"Processing videos...\"):\n            process_video(\n                video,\n                output_path,\n                worker_cfg=worker_cfg,\n                is_url=is_url,\n                sampler_cls=sampler_cls,\n            )\n    else:\n        futures = []\n        with ProcessPoolExecutor(max_workers=n_workers) as executor:\n            console.print(\n                f\"Using {executor._max_workers} workers\",\n                style=f\"bold {Color.green.value}\",\n            )\n            executor._max_workers\n            for video in video_iterable:\n                futures.append(\n                    executor.submit(\n                        process_video,\n                        video,\n                        output_path,\n                        is_url=is_url,\n                        worker_cfg=worker_cfg,\n                        sampler_cls=sampler_cls,\n                    )\n                )\n            for future in tqdm(as_completed(futures), desc=\"Processing videos...\"):\n                future.result()\n</code></pre>"},{"location":"reference/video_sampler/iterators/#video_sampler.iterators.process_video","title":"<code>process_video(video_info, output_path, is_url, worker_cfg, sampler_cls=VideoSampler)</code>","text":"<p>Process a video file or URL.</p> <p>Parameters:</p> Name Type Description Default <code>video_info</code> <code>Union[str, tuple]</code> <p>A video file path or a tuple containing the video title, URL and subtitles.</p> required <code>output_path</code> <code>str</code> <p>Path to the output folder.</p> required <code>worker</code> <code>Worker</code> <p>Worker instance to process the videos.</p> required <code>is_url</code> <code>bool</code> <p>Flag to indicate if the video is a URL.</p> required Source code in <code>video_sampler/iterators.py</code> <pre><code>def process_video(\n    video_info: str | tuple[str, str, str | None],\n    output_path: str,\n    is_url: bool,\n    worker_cfg: SamplerConfig | ImageSamplerConfig,\n    sampler_cls: BaseSampler | None = VideoSampler,\n):\n    \"\"\"Process a video file or URL.\n\n    Args:\n        video_info (Union[str, tuple]): A video file path or a tuple containing the video title,\n            URL and subtitles.\n        output_path (str): Path to the output folder.\n        worker (Worker): Worker instance to process the videos.\n        is_url (bool): Flag to indicate if the video is a URL.\n    \"\"\"\n    worker = Worker(cfg=worker_cfg, sampler_cls=sampler_cls)\n    try:\n        if is_url:\n            video_title, video_url, subs = video_info\n            video_filename = slugify(video_title)\n            video_subpath = os.path.join(output_path, video_filename)\n            worker.launch(\n                video_path=video_url,\n                output_path=video_subpath,\n                pretty_video_name=video_filename,\n                subs=subs,\n            )\n        else:\n            if isinstance(worker_cfg, ImageSamplerConfig):\n                video_subpath = output_path\n            else:\n                video_subpath = os.path.join(output_path, os.path.basename(video_info))\n            worker.launch(\n                video_path=video_info,\n                output_path=video_subpath,\n            )\n    except Exception as e:\n        console.print(\n            f\"Error processing video {video_info}: {e}\", style=f\"bold {Color.red.value}\"\n        )\n</code></pre>"},{"location":"reference/video_sampler/logging/","title":"Logging","text":""},{"location":"reference/video_sampler/schemas/","title":"Schemas","text":""},{"location":"reference/video_sampler/schemas/#video_sampler.schemas.FrameObject","title":"<code>FrameObject</code>  <code>dataclass</code>","text":"<p>A frame object.</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>Image</code> <p>The frame image.</p> required <code>metadata</code> <code>dict</code> <p>The metadata for the frame.</p> required Source code in <code>video_sampler/schemas.py</code> <pre><code>@dataclass\nclass FrameObject:\n    \"\"\"\n    A frame object.\n\n    Args:\n        frame (Image.Image): The frame image.\n        metadata (dict): The metadata for the frame.\n    \"\"\"\n\n    frame: Image.Image\n    metadata: dict\n</code></pre>"},{"location":"reference/video_sampler/ttl_counter/","title":"Ttl counter","text":""},{"location":"reference/video_sampler/ttl_counter/#video_sampler.ttl_counter.TTLCounter","title":"<code>TTLCounter</code>","text":"<p>TTLCounter is a counter/list that expires items after a TTL period expires.</p> Source code in <code>video_sampler/ttl_counter.py</code> <pre><code>class TTLCounter:\n    \"\"\"TTLCounter is a counter/list that expires items after a TTL period expires.\"\"\"\n\n    def __init__(self, max_ttl: int) -&gt; None:\n        self.inner_counter = []\n        self.max_ttl = max_ttl\n\n    def __len__(self):\n        \"\"\"Return the number of items in the counter.\"\"\"\n        return len(self.inner_counter)\n\n    def add_item(self, hash: str):\n        \"\"\"Add an item with the max TTL.\"\"\"\n        heapq.heappush(self.inner_counter, (self.max_ttl, hash))\n\n    def tick(self):\n        \"\"\"Decrease the TTL of all items by 1.\"\"\"\n        for i, (ttl, hash) in enumerate(self.inner_counter):\n            self.inner_counter[i] = (ttl - 1, hash)\n\n    def expire_one(self):\n        \"\"\"Expire the first item if its TTL is 0. Expires AT MOST one item.\"\"\"\n        # peek the first item\n        ttl, hash = self.inner_counter[0]\n        if ttl &lt;= 0:\n            heapq.heappop(self.inner_counter)\n            return hash\n        return None\n\n    def expire_all(self):\n        \"\"\"Expire all items.\"\"\"\n        for _, hash in self.inner_counter:\n            yield hash\n        self.inner_counter.clear()\n</code></pre>"},{"location":"reference/video_sampler/ttl_counter/#video_sampler.ttl_counter.TTLCounter.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of items in the counter.</p> Source code in <code>video_sampler/ttl_counter.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the number of items in the counter.\"\"\"\n    return len(self.inner_counter)\n</code></pre>"},{"location":"reference/video_sampler/ttl_counter/#video_sampler.ttl_counter.TTLCounter.add_item","title":"<code>add_item(hash)</code>","text":"<p>Add an item with the max TTL.</p> Source code in <code>video_sampler/ttl_counter.py</code> <pre><code>def add_item(self, hash: str):\n    \"\"\"Add an item with the max TTL.\"\"\"\n    heapq.heappush(self.inner_counter, (self.max_ttl, hash))\n</code></pre>"},{"location":"reference/video_sampler/ttl_counter/#video_sampler.ttl_counter.TTLCounter.expire_all","title":"<code>expire_all()</code>","text":"<p>Expire all items.</p> Source code in <code>video_sampler/ttl_counter.py</code> <pre><code>def expire_all(self):\n    \"\"\"Expire all items.\"\"\"\n    for _, hash in self.inner_counter:\n        yield hash\n    self.inner_counter.clear()\n</code></pre>"},{"location":"reference/video_sampler/ttl_counter/#video_sampler.ttl_counter.TTLCounter.expire_one","title":"<code>expire_one()</code>","text":"<p>Expire the first item if its TTL is 0. Expires AT MOST one item.</p> Source code in <code>video_sampler/ttl_counter.py</code> <pre><code>def expire_one(self):\n    \"\"\"Expire the first item if its TTL is 0. Expires AT MOST one item.\"\"\"\n    # peek the first item\n    ttl, hash = self.inner_counter[0]\n    if ttl &lt;= 0:\n        heapq.heappop(self.inner_counter)\n        return hash\n    return None\n</code></pre>"},{"location":"reference/video_sampler/ttl_counter/#video_sampler.ttl_counter.TTLCounter.tick","title":"<code>tick()</code>","text":"<p>Decrease the TTL of all items by 1.</p> Source code in <code>video_sampler/ttl_counter.py</code> <pre><code>def tick(self):\n    \"\"\"Decrease the TTL of all items by 1.\"\"\"\n    for i, (ttl, hash) in enumerate(self.inner_counter):\n        self.inner_counter[i] = (ttl - 1, hash)\n</code></pre>"},{"location":"reference/video_sampler/utils/","title":"Utils","text":""},{"location":"reference/video_sampler/utils/#video_sampler.utils.batched","title":"<code>batched(iterable, n)</code>","text":"<p>Batch data into tuples of length n. The last batch may be shorter. from https://docs.python.org/3/library/itertools.html#itertools-recipes</p> Source code in <code>video_sampler/utils.py</code> <pre><code>def batched(iterable, n):\n    \"\"\"\n    Batch data into tuples of length n. The last batch may be shorter.\n    from https://docs.python.org/3/library/itertools.html#itertools-recipes\n    \"\"\"\n    if n &lt; 1:\n        raise ValueError(\"n must be at least one\")\n    it = iter(iterable)\n    while batch := tuple(islice(it, n)):\n        yield batch\n</code></pre>"},{"location":"reference/video_sampler/utils/#video_sampler.utils.slugify","title":"<code>slugify(value, allow_unicode=False)</code>","text":"<p>Taken from https://github.com/django/django/blob/master/django/utils/text.py Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated dashes to single dashes. Remove characters that aren't alphanumerics, underscores, or hyphens. Convert to lowercase. Also strip leading and trailing whitespace, dashes, and underscores.</p> Source code in <code>video_sampler/utils.py</code> <pre><code>def slugify(value, allow_unicode=False):\n    \"\"\"\n    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n    dashes to single dashes. Remove characters that aren't alphanumerics,\n    underscores, or hyphens. Convert to lowercase. Also strip leading and\n    trailing whitespace, dashes, and underscores.\n    \"\"\"\n    value = str(value)\n    if allow_unicode:\n        value = unicodedata.normalize(\"NFKC\", value)\n    else:\n        value = (\n            unicodedata.normalize(\"NFKD\", value)\n            .encode(\"ascii\", \"ignore\")\n            .decode(\"ascii\")\n        )\n    value = re.sub(r\"[^\\w\\s-]\", \"\", value.lower())\n    return re.sub(r\"[-\\s]+\", \"-\", value).strip(\"-_\")\n</code></pre>"},{"location":"reference/video_sampler/worker/","title":"Worker","text":""},{"location":"reference/video_sampler/worker/#video_sampler.worker.Worker","title":"<code>Worker</code>","text":"Source code in <code>video_sampler/worker.py</code> <pre><code>class Worker:\n    def __init__(\n        self,\n        cfg: SamplerConfig,\n        devnull: bool = False,\n        sampler_cls: VideoSampler = VideoSampler,\n        extra_sampler_args: dict | None = None,\n    ) -&gt; None:\n        if extra_sampler_args is None:\n            extra_sampler_args = {}\n        self.cfg: SamplerConfig = cfg\n        self.sampler: VideoSampler = sampler_cls(cfg=cfg, **extra_sampler_args)\n        self.q = Queue()\n        self.devnull = devnull\n        self.__initialise_summary_objs()\n\n    def __initialise_summary_objs(self):\n        self.pool = None\n        self.futures = {}\n        if self.cfg.summary_config:\n            from concurrent.futures import ThreadPoolExecutor\n\n            from .integrations.llava_chat import ImageDescriptionDefault\n\n            console.print(\"Initialising summary pool...\", style=\"bold yellow\")\n            self.pool = ThreadPoolExecutor(\n                max_workers=self.cfg.summary_config.get(\"max_workers\", 2)\n            )\n            self.desc_client = ImageDescriptionDefault(\n                url=self.cfg.summary_config.get(\"url\")\n            )\n\n    def collect_summaries(self, savepath: str):\n        if not self.pool:\n            return\n        console.print(\n            f\"Waiting for summary pool to finish: [{len(self.futures)}] items...\",\n            style=\"bold yellow\",\n        )\n        summary_info = []\n        for k, v in self.futures.items():\n            if summary := v.result():\n                summary_info.append({\"time\": k, \"summary\": summary})\n                if self.cfg.debug:\n                    console.print(\n                        f\"Summary for frame {k}\",\n                        f\"\\t{summary}\",\n                        style=\"bold green\",\n                    )\n\n        # save as a jsonl\n        try:\n            with open(os.path.join(savepath, \"summaries.jsonl\"), \"w\") as f:\n                for item in summary_info:\n                    f.write(json.dumps(item) + \"\\n\")\n        except OSError as e:\n            console.print(f\"Failed to write to file: {e}\", style=\"bold red\")\n\n    def launch(\n        self,\n        video_path: str,\n        output_path: str = \"\",\n        pretty_video_name: str = \"\",\n        subs: str = None,\n    ) -&gt; None:\n        \"\"\"\n        Launch the worker.\n\n        Args:\n            video_path (str): Path to the video file.\n            output_path (str, optional): Path to the output folder. Defaults to \"\".\n            pretty_video_name (str, optional): Name of the video file for pretty printing (useful for urls).\n                                                Defaults to \"\".\n        \"\"\"\n        if not pretty_video_name:\n            pretty_video_name = os.path.basename(video_path)\n        if output_path and self.devnull:\n            raise ValueError(\"Cannot write to disk when devnull is True\")\n        if isinstance(self.cfg, ImageSamplerConfig):\n            output_path = os.path.join(output_path, os.path.basename(video_path))\n        if output_path:\n            os.makedirs(output_path, exist_ok=True)\n        proc_thread = Thread(\n            target=self.sampler.write_queue, args=(video_path, self.q, subs)\n        )\n        proc_thread.start()\n        self.queue_reader(output_path, read_interval=self.cfg.queue_wait)\n        proc_thread.join()\n        self.collect_summaries(output_path)\n        if self.cfg.print_stats:\n            console.print(\n                f\"Stats for: {pretty_video_name}\",\n                f\"\\n\\tTotal frames: {self.sampler.stats['total']}\",\n                f\"\\n\\tDecoded frames: {self.sampler.stats['decoded']}\",\n                f\"\\n\\tProduced frames: {self.sampler.stats['produced']}\",\n                f\"\\n\\tGated frames: {self.sampler.stats['gated']}\",\n                style=f\"bold {Color.magenta.value}\",\n            )\n\n    def format_output_path(self, output_path: str, frame_time: float) -&gt; str:\n        \"\"\"Format the output path for a frame.\"\"\"\n        ft = str(frame_time)\n        if self.cfg.save_format.encode_time_b64:\n            ft = base64.encodebytes(ft.encode()).decode().rstrip()\n            ft = f\"TIMEB64_{ft}\"\n        elif self.cfg.save_format.avoid_dot:\n            ft = ft.replace(\".\", \"_\")\n            ft = f\"TIMESEC_{ft}\"\n        if self.cfg.save_format.include_filename:\n            vbsn = os.path.basename(output_path)\n            # remove extension\n            vbsn = os.path.splitext(vbsn)[0]\n            ft = f\"{vbsn}_{ft}\"\n        return os.path.join(output_path, f\"{ft}.jpg\")\n\n    def queue_reader(self, output_path, read_interval=0.1) -&gt; None:\n        \"\"\"\n        Reads frames from the queue and saves them as JPEG images.\n\n        Args:\n            output_path (str): The directory path where the frames will be saved.\n            read_interval (float, optional): The time interval between reading frames from the queue.\n                    Defaults to 0.1 seconds.\n        \"\"\"\n        last_summary_time = -10\n        self.futures = {}  # clear futures\n        while True:\n            if not self.q.empty():\n                frame_object: FrameObject\n                for frame_object in self.q.get():\n                    if frame_object.metadata.get(\"end\", False):\n                        return\n                    if frame_object.frame is not None and (\n                        not self.devnull and isinstance(frame_object.frame, Image.Image)\n                    ):\n                        frame_object.frame.save(\n                            self.format_output_path(\n                                output_path, frame_object.metadata[\"frame_time\"]\n                            )\n                        )\n                        if self.pool:\n                            ftime = frame_object.metadata[\"frame_time\"]\n                            if ftime - last_summary_time &lt; self.cfg.summary_config.get(\n                                \"min_sum_interval\", 30\n                            ):  # seconds\n                                continue\n\n                            future = self.pool.submit(\n                                self.desc_client.summarise_image, frame_object.frame\n                            )\n                            if self.cfg.debug:\n                                console.print(\n                                    f\"Submitting summary for frame {ftime}\",\n                                    style=\"bold yellow\",\n                                )\n                            self.futures[ftime] = future\n                            last_summary_time = ftime\n            time.sleep(read_interval)\n</code></pre>"},{"location":"reference/video_sampler/worker/#video_sampler.worker.Worker.format_output_path","title":"<code>format_output_path(output_path, frame_time)</code>","text":"<p>Format the output path for a frame.</p> Source code in <code>video_sampler/worker.py</code> <pre><code>def format_output_path(self, output_path: str, frame_time: float) -&gt; str:\n    \"\"\"Format the output path for a frame.\"\"\"\n    ft = str(frame_time)\n    if self.cfg.save_format.encode_time_b64:\n        ft = base64.encodebytes(ft.encode()).decode().rstrip()\n        ft = f\"TIMEB64_{ft}\"\n    elif self.cfg.save_format.avoid_dot:\n        ft = ft.replace(\".\", \"_\")\n        ft = f\"TIMESEC_{ft}\"\n    if self.cfg.save_format.include_filename:\n        vbsn = os.path.basename(output_path)\n        # remove extension\n        vbsn = os.path.splitext(vbsn)[0]\n        ft = f\"{vbsn}_{ft}\"\n    return os.path.join(output_path, f\"{ft}.jpg\")\n</code></pre>"},{"location":"reference/video_sampler/worker/#video_sampler.worker.Worker.launch","title":"<code>launch(video_path, output_path='', pretty_video_name='', subs=None)</code>","text":"<p>Launch the worker.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to the video file.</p> required <code>output_path</code> <code>str</code> <p>Path to the output folder. Defaults to \"\".</p> <code>''</code> <code>pretty_video_name</code> <code>str</code> <p>Name of the video file for pretty printing (useful for urls).                                 Defaults to \"\".</p> <code>''</code> Source code in <code>video_sampler/worker.py</code> <pre><code>def launch(\n    self,\n    video_path: str,\n    output_path: str = \"\",\n    pretty_video_name: str = \"\",\n    subs: str = None,\n) -&gt; None:\n    \"\"\"\n    Launch the worker.\n\n    Args:\n        video_path (str): Path to the video file.\n        output_path (str, optional): Path to the output folder. Defaults to \"\".\n        pretty_video_name (str, optional): Name of the video file for pretty printing (useful for urls).\n                                            Defaults to \"\".\n    \"\"\"\n    if not pretty_video_name:\n        pretty_video_name = os.path.basename(video_path)\n    if output_path and self.devnull:\n        raise ValueError(\"Cannot write to disk when devnull is True\")\n    if isinstance(self.cfg, ImageSamplerConfig):\n        output_path = os.path.join(output_path, os.path.basename(video_path))\n    if output_path:\n        os.makedirs(output_path, exist_ok=True)\n    proc_thread = Thread(\n        target=self.sampler.write_queue, args=(video_path, self.q, subs)\n    )\n    proc_thread.start()\n    self.queue_reader(output_path, read_interval=self.cfg.queue_wait)\n    proc_thread.join()\n    self.collect_summaries(output_path)\n    if self.cfg.print_stats:\n        console.print(\n            f\"Stats for: {pretty_video_name}\",\n            f\"\\n\\tTotal frames: {self.sampler.stats['total']}\",\n            f\"\\n\\tDecoded frames: {self.sampler.stats['decoded']}\",\n            f\"\\n\\tProduced frames: {self.sampler.stats['produced']}\",\n            f\"\\n\\tGated frames: {self.sampler.stats['gated']}\",\n            style=f\"bold {Color.magenta.value}\",\n        )\n</code></pre>"},{"location":"reference/video_sampler/worker/#video_sampler.worker.Worker.queue_reader","title":"<code>queue_reader(output_path, read_interval=0.1)</code>","text":"<p>Reads frames from the queue and saves them as JPEG images.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>The directory path where the frames will be saved.</p> required <code>read_interval</code> <code>float</code> <p>The time interval between reading frames from the queue.     Defaults to 0.1 seconds.</p> <code>0.1</code> Source code in <code>video_sampler/worker.py</code> <pre><code>def queue_reader(self, output_path, read_interval=0.1) -&gt; None:\n    \"\"\"\n    Reads frames from the queue and saves them as JPEG images.\n\n    Args:\n        output_path (str): The directory path where the frames will be saved.\n        read_interval (float, optional): The time interval between reading frames from the queue.\n                Defaults to 0.1 seconds.\n    \"\"\"\n    last_summary_time = -10\n    self.futures = {}  # clear futures\n    while True:\n        if not self.q.empty():\n            frame_object: FrameObject\n            for frame_object in self.q.get():\n                if frame_object.metadata.get(\"end\", False):\n                    return\n                if frame_object.frame is not None and (\n                    not self.devnull and isinstance(frame_object.frame, Image.Image)\n                ):\n                    frame_object.frame.save(\n                        self.format_output_path(\n                            output_path, frame_object.metadata[\"frame_time\"]\n                        )\n                    )\n                    if self.pool:\n                        ftime = frame_object.metadata[\"frame_time\"]\n                        if ftime - last_summary_time &lt; self.cfg.summary_config.get(\n                            \"min_sum_interval\", 30\n                        ):  # seconds\n                            continue\n\n                        future = self.pool.submit(\n                            self.desc_client.summarise_image, frame_object.frame\n                        )\n                        if self.cfg.debug:\n                            console.print(\n                                f\"Submitting summary for frame {ftime}\",\n                                style=\"bold yellow\",\n                            )\n                        self.futures[ftime] = future\n                        last_summary_time = ftime\n        time.sleep(read_interval)\n</code></pre>"},{"location":"reference/video_sampler/integrations/llava_chat/","title":"Integrations","text":""},{"location":"reference/video_sampler/integrations/llava_chat/#video_sampler.integrations.llava_chat.ImageDescription","title":"<code>ImageDescription</code>","text":"<p>A client to interact with the image description API. The API is used to generate short phrases that describe an image.</p> <p>Methods:</p> Name Description <code>summarise_image</code> <p>Image) -&gt; str: Summarise the image using the LLaMA API.</p> Source code in <code>video_sampler/integrations/llava_chat.py</code> <pre><code>class ImageDescription:\n    \"\"\"A client to interact with the image description API.\n    The API is used to generate short phrases that describe an image.\n\n    Methods:\n        summarise_image(image: Image) -&gt; str:\n            Summarise the image using the LLaMA API.\n    \"\"\"\n\n    def __init__(self, url: str) -&gt; None:\n        if url is None:\n            url = \"http://localhost:8080/\"\n        self.url = url\n\n    def summarise_image(self, image: Image) -&gt; str:\n        \"\"\"Summarise the image\n        Args:\n            image (Image): The image to summarise.\n        Returns:\n            str: The description of the image.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/video_sampler/integrations/llava_chat/#video_sampler.integrations.llava_chat.ImageDescription.summarise_image","title":"<code>summarise_image(image)</code>","text":"<p>Summarise the image Args:     image (Image): The image to summarise. Returns:     str: The description of the image.</p> Source code in <code>video_sampler/integrations/llava_chat.py</code> <pre><code>def summarise_image(self, image: Image) -&gt; str:\n    \"\"\"Summarise the image\n    Args:\n        image (Image): The image to summarise.\n    Returns:\n        str: The description of the image.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/video_sampler/integrations/llava_chat/#video_sampler.integrations.llava_chat.ImageDescriptionDefault","title":"<code>ImageDescriptionDefault</code>","text":"<p>             Bases: <code>ImageDescription</code></p> <p>A client to interact with the LLaMA image description API. The API is used to generate short phrases that describe an image.</p> <p>Methods:</p> Name Description <code>summarise_image</code> <p>Image) -&gt; str: Summarise the image using the LLaMA API.</p> Source code in <code>video_sampler/integrations/llava_chat.py</code> <pre><code>class ImageDescriptionDefault(ImageDescription):\n    \"\"\"A client to interact with the LLaMA image description API.\n    The API is used to generate short phrases that describe an image.\n\n    Methods:\n        summarise_image(image: Image) -&gt; str:\n            Summarise the image using the LLaMA API.\n    \"\"\"\n\n    def __init__(self, url: str = \"http://localhost:8080/completion\"):\n        \"\"\"Initialise the client with the base URL of the LLaMA API.\n        Args:\n            url (str): The base URL of the LLaMA API.\n        \"\"\"\n        \"\"\"TODO: migrate to OpenAI API when available\"\"\"\n        super().__init__(url)\n        self.headers = {\n            \"accept-language\": \"en-GB,en\",\n            \"content-type\": \"application/json\",\n        }\n        if api_key := os.getenv(\"OPENAI_API_KEY\"):\n            self.headers[\"Authorization\"] = f\"Bearer {api_key}\"\n        self.session = requests.Session()\n\n    def get_prompt(self):\n        return \"\"\"You're an AI assistant that describes images using short phrases.\n        The image is shown below.\n        \\nIMAGE:[img-10]\n        \\nASSISTANT:\"\"\"\n\n    def summarise_image(self, image: Image) -&gt; str:\n        \"\"\"Summarise the image using the LLaMA API.\n        Args:\n            image (Image): The image to summarise.\n        Returns:\n            str: The description of the image.\n        \"\"\"\n        b64image = encode_image(resize_image(image))\n\n        json_body = {\n            \"model\": os.getenv(\"OPENAI_MODEL\", \"LLaVA_CPP\"),\n            \"stream\": False,\n            \"n_predict\": 1000,\n            \"temperature\": 0.1,\n            \"repeat_last_n\": 78,\n            \"image_data\": [{\"data\": b64image, \"id\": 10}],\n            \"cache_prompt\": True,\n            \"top_k\": 40,\n            \"top_p\": 1,\n            \"min_p\": 0.05,\n            \"tfs_z\": 1,\n            \"typical_p\": 1,\n            \"presence_penalty\": 0,\n            \"frequency_penalty\": 0,\n            \"mirostat\": 0,\n            \"mirostat_tau\": 5,\n            \"mirostat_eta\": 0.1,\n            \"grammar\": \"\",\n            \"n_probs\": 0,\n            \"min_keep\": 0,\n            \"api_key\": os.getenv(\"OPENAI_API_KEY\", \"\"),\n            \"slot_id\": 0,\n            \"stop\": [\"&lt;/s&gt;\", \"Llama:\", \"User:\"],\n            \"prompt\": self.get_prompt(),\n        }\n        response = self.session.post(\n            f\"{self.url}\",\n            json=json_body,\n            headers=self.headers,\n            stream=False,\n        )\n        if response.status_code != 200:\n            print(f\"Failed to summarise image: {response}\")\n            return None\n        res = response.json()\n        if \"choices\" in res:\n            return res[\"choices\"][0][\"text\"].strip()\n        elif \"content\" in res:\n            return res[\"content\"].strip()\n        raise ValueError(f\"Failed to summarise image: unknown response format: {res}\")\n</code></pre>"},{"location":"reference/video_sampler/integrations/llava_chat/#video_sampler.integrations.llava_chat.ImageDescriptionDefault.__init__","title":"<code>__init__(url='http://localhost:8080/completion')</code>","text":"<p>Initialise the client with the base URL of the LLaMA API. Args:     url (str): The base URL of the LLaMA API.</p> Source code in <code>video_sampler/integrations/llava_chat.py</code> <pre><code>def __init__(self, url: str = \"http://localhost:8080/completion\"):\n    \"\"\"Initialise the client with the base URL of the LLaMA API.\n    Args:\n        url (str): The base URL of the LLaMA API.\n    \"\"\"\n    \"\"\"TODO: migrate to OpenAI API when available\"\"\"\n    super().__init__(url)\n    self.headers = {\n        \"accept-language\": \"en-GB,en\",\n        \"content-type\": \"application/json\",\n    }\n    if api_key := os.getenv(\"OPENAI_API_KEY\"):\n        self.headers[\"Authorization\"] = f\"Bearer {api_key}\"\n    self.session = requests.Session()\n</code></pre>"},{"location":"reference/video_sampler/integrations/llava_chat/#video_sampler.integrations.llava_chat.ImageDescriptionDefault.summarise_image","title":"<code>summarise_image(image)</code>","text":"<p>Summarise the image using the LLaMA API. Args:     image (Image): The image to summarise. Returns:     str: The description of the image.</p> Source code in <code>video_sampler/integrations/llava_chat.py</code> <pre><code>def summarise_image(self, image: Image) -&gt; str:\n    \"\"\"Summarise the image using the LLaMA API.\n    Args:\n        image (Image): The image to summarise.\n    Returns:\n        str: The description of the image.\n    \"\"\"\n    b64image = encode_image(resize_image(image))\n\n    json_body = {\n        \"model\": os.getenv(\"OPENAI_MODEL\", \"LLaVA_CPP\"),\n        \"stream\": False,\n        \"n_predict\": 1000,\n        \"temperature\": 0.1,\n        \"repeat_last_n\": 78,\n        \"image_data\": [{\"data\": b64image, \"id\": 10}],\n        \"cache_prompt\": True,\n        \"top_k\": 40,\n        \"top_p\": 1,\n        \"min_p\": 0.05,\n        \"tfs_z\": 1,\n        \"typical_p\": 1,\n        \"presence_penalty\": 0,\n        \"frequency_penalty\": 0,\n        \"mirostat\": 0,\n        \"mirostat_tau\": 5,\n        \"mirostat_eta\": 0.1,\n        \"grammar\": \"\",\n        \"n_probs\": 0,\n        \"min_keep\": 0,\n        \"api_key\": os.getenv(\"OPENAI_API_KEY\", \"\"),\n        \"slot_id\": 0,\n        \"stop\": [\"&lt;/s&gt;\", \"Llama:\", \"User:\"],\n        \"prompt\": self.get_prompt(),\n    }\n    response = self.session.post(\n        f\"{self.url}\",\n        json=json_body,\n        headers=self.headers,\n        stream=False,\n    )\n    if response.status_code != 200:\n        print(f\"Failed to summarise image: {response}\")\n        return None\n    res = response.json()\n    if \"choices\" in res:\n        return res[\"choices\"][0][\"text\"].strip()\n    elif \"content\" in res:\n        return res[\"content\"].strip()\n    raise ValueError(f\"Failed to summarise image: unknown response format: {res}\")\n</code></pre>"},{"location":"reference/video_sampler/integrations/llava_chat/#video_sampler.integrations.llava_chat.VideoSummary","title":"<code>VideoSummary</code>","text":"<p>A client to interact with the LLaMA video summarisation API. The API is used to generate a summary of a video based on image descriptions.</p> <p>Methods:</p> Name Description <code>summarise_video</code> <p>list[str]) -&gt; str: Summarise the video using the LLaMA API.</p> Source code in <code>video_sampler/integrations/llava_chat.py</code> <pre><code>class VideoSummary:\n    \"\"\"A client to interact with the LLaMA video summarisation API.\n    The API is used to generate a summary of a video based on image descriptions.\n\n    Methods:\n        summarise_video(image_descriptions: list[str]) -&gt; str:\n            Summarise the video using the LLaMA API.\n    \"\"\"\n\n    def __init__(self, url: str | None = \"http://localhost:8080/v1\"):\n        \"\"\"Initialise the client with the base URL of the LLaMA API.\n        Args:\n            url (str): The base URL of the LLaMA API.\"\"\"\n        self.url = url if url is not None else \"http://localhost:8080/v1\"\n        self.client = OpenAI(base_url=self.url)\n\n    def get_prompt(self):\n        return \"\"\"You're an AI assistant that summarises videos based on image descriptions.\n        Combine image descriptions into a coherent summary of the video.\"\"\"\n\n    def summarise_video(self, image_descriptions: list[str]):\n        \"\"\"Summarise the video using the LLaMA API.\n        Args:\n            image_descriptions (list[str]): The descriptions of the images in the video.\n        Returns:\n            str: The summary of the video.\n        \"\"\"\n        return self.client.chat.completions.create(\n            model=\"LLaMA_CPP\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": self.get_prompt(),\n                },\n                {\"role\": \"user\", \"content\": \"\\n\".join(image_descriptions)},\n            ],\n            max_tokens=300,\n        )\n</code></pre>"},{"location":"reference/video_sampler/integrations/llava_chat/#video_sampler.integrations.llava_chat.VideoSummary.__init__","title":"<code>__init__(url='http://localhost:8080/v1')</code>","text":"<p>Initialise the client with the base URL of the LLaMA API. Args:     url (str): The base URL of the LLaMA API.</p> Source code in <code>video_sampler/integrations/llava_chat.py</code> <pre><code>def __init__(self, url: str | None = \"http://localhost:8080/v1\"):\n    \"\"\"Initialise the client with the base URL of the LLaMA API.\n    Args:\n        url (str): The base URL of the LLaMA API.\"\"\"\n    self.url = url if url is not None else \"http://localhost:8080/v1\"\n    self.client = OpenAI(base_url=self.url)\n</code></pre>"},{"location":"reference/video_sampler/integrations/llava_chat/#video_sampler.integrations.llava_chat.VideoSummary.summarise_video","title":"<code>summarise_video(image_descriptions)</code>","text":"<p>Summarise the video using the LLaMA API. Args:     image_descriptions (list[str]): The descriptions of the images in the video. Returns:     str: The summary of the video.</p> Source code in <code>video_sampler/integrations/llava_chat.py</code> <pre><code>def summarise_video(self, image_descriptions: list[str]):\n    \"\"\"Summarise the video using the LLaMA API.\n    Args:\n        image_descriptions (list[str]): The descriptions of the images in the video.\n    Returns:\n        str: The summary of the video.\n    \"\"\"\n    return self.client.chat.completions.create(\n        model=\"LLaMA_CPP\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": self.get_prompt(),\n            },\n            {\"role\": \"user\", \"content\": \"\\n\".join(image_descriptions)},\n        ],\n        max_tokens=300,\n    )\n</code></pre>"},{"location":"reference/video_sampler/integrations/llava_chat/#video_sampler.integrations.llava_chat.encode_image","title":"<code>encode_image(image)</code>","text":"<p>Convert the image to base64</p> Source code in <code>video_sampler/integrations/llava_chat.py</code> <pre><code>def encode_image(image: Image):\n    \"\"\"\n    Convert the image to base64\n    \"\"\"\n    # create a buffer to store the image\n    buffer = io.BytesIO()\n    # save the image to the buffer\n    image.save(buffer, format=\"JPEG\")\n    # convert the image to base64\n    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n</code></pre>"},{"location":"reference/video_sampler/integrations/llava_chat/#video_sampler.integrations.llava_chat.resize_image","title":"<code>resize_image(image, max_side=512)</code>","text":"<p>Resize the image to max_side if any of the sides is greater than max_side. If max_side is None, the image is returned as is.</p> Source code in <code>video_sampler/integrations/llava_chat.py</code> <pre><code>def resize_image(image: Image, max_side: int = 512):\n    \"\"\"\n    Resize the image to max_side if any of the sides is greater than max_side.\n    If max_side is None, the image is returned as is.\n    \"\"\"\n    # get the image shape\n    if max_side is None:\n        return image\n    width, height = image.size\n    if max(width, height) &gt; max_side:\n        # resize the image to max_side\n        # keeping the aspect ratio\n        if width &gt; height:\n            new_width = max_side\n            new_height = int(height * max_side / width)\n        else:\n            new_height = max_side\n            new_width = int(width * max_side / height)\n        return image.resize((new_width, new_height))\n    return image\n</code></pre>"},{"location":"reference/video_sampler/integrations/yt_dlp_plugin/","title":"Yt dlp plugin","text":""},{"location":"reference/video_sampler/integrations/yt_dlp_plugin/#video_sampler.integrations.yt_dlp_plugin.YTDLPPlugin","title":"<code>YTDLPPlugin</code>","text":"<p>A plugin for yt-dlp to generate URLs and corresponding titles from the given URL.</p> <p>Methods:</p> Name Description <code>generate_urls</code> <p>Generates URLs and corresponding titles from the given URL.</p> Source code in <code>video_sampler/integrations/yt_dlp_plugin.py</code> <pre><code>class YTDLPPlugin:\n    \"\"\"\n    A plugin for yt-dlp to generate URLs and corresponding titles from the given URL.\n\n    Methods:\n        generate_urls(url, extra_yt_constr_args=None, extra_info_extract_opts=None) -&gt; Iterable[str]:\n            Generates URLs and corresponding titles from the given URL.\n\n    \"\"\"\n\n    def __init__(self, ie_key: str = \"Generic\"):\n        \"\"\"\n        Initialize the YTDLPPlugin instance.\n        \"\"\"\n        self.ie_key = ie_key\n        self.ydl_opts = {\n            \"format\": best_video_only,\n        }\n\n    def generate_urls(\n        self,\n        url: str,\n        extra_info_extract_opts: dict = None,\n        get_subs: bool = False,\n    ) -&gt; Iterable[tuple[str, str, str | None]]:\n        \"\"\"Generate URLs and download subtitles for a given video URL.\n\n        Args:\n            url (str): The URL of the video to download subtitles for.\n            extra_info_extract_opts (dict, optional): Additional options for extracting video information.\n\n        Yields:\n            tuple: A tuple containing the video title, video format URL, and downloaded subtitles.\n        \"\"\"\n        if extra_info_extract_opts is None:\n            extra_info_extract_opts = {}\n        if get_subs:\n            extra_info_extract_opts |= self.get_subtitles_opts()\n        extr_args = {\"ie_key\": self.ie_key} if \"ytsearch\" not in url else {}\n\n        def preproc_entry(info):\n            req_format = info[\"requested_formats\"][0]\n            subs = None\n            if get_subs and \"requested_subtitles\" in info:\n                subs = download_sub(\n                    list(info[\"requested_subtitles\"].values())[0][\"url\"]\n                )\n            return info[\"title\"], req_format[\"url\"], subs\n\n        with YoutubeDL(params=(self.ydl_opts | extra_info_extract_opts)) as ydl:\n            info = ydl.extract_info(url, download=False, **extr_args)\n            if \"entries\" not in info:\n                yield preproc_entry(info)\n            else:\n                for entry in info.get(\"entries\", []):\n                    if not entry:\n                        continue\n                    yield preproc_entry(entry)\n\n    def get_subtitles_opts(self) -&gt; dict:\n        return {\n            \"postprocessors\": [\n                {\n                    \"format\": \"srt\",\n                    \"key\": \"FFmpegSubtitlesConvertor\",\n                    \"when\": \"before_dl\",\n                }\n            ],\n            \"format\": best_video_only,\n            \"subtitleslangs\": [\"en.*\"],\n            \"writeautomaticsub\": True,\n            \"writesubtitles\": True,\n        }\n</code></pre>"},{"location":"reference/video_sampler/integrations/yt_dlp_plugin/#video_sampler.integrations.yt_dlp_plugin.YTDLPPlugin.__init__","title":"<code>__init__(ie_key='Generic')</code>","text":"<p>Initialize the YTDLPPlugin instance.</p> Source code in <code>video_sampler/integrations/yt_dlp_plugin.py</code> <pre><code>def __init__(self, ie_key: str = \"Generic\"):\n    \"\"\"\n    Initialize the YTDLPPlugin instance.\n    \"\"\"\n    self.ie_key = ie_key\n    self.ydl_opts = {\n        \"format\": best_video_only,\n    }\n</code></pre>"},{"location":"reference/video_sampler/integrations/yt_dlp_plugin/#video_sampler.integrations.yt_dlp_plugin.YTDLPPlugin.generate_urls","title":"<code>generate_urls(url, extra_info_extract_opts=None, get_subs=False)</code>","text":"<p>Generate URLs and download subtitles for a given video URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the video to download subtitles for.</p> required <code>extra_info_extract_opts</code> <code>dict</code> <p>Additional options for extracting video information.</p> <code>None</code> <p>Yields:</p> Name Type Description <code>tuple</code> <code>Iterable[tuple[str, str, str | None]]</code> <p>A tuple containing the video title, video format URL, and downloaded subtitles.</p> Source code in <code>video_sampler/integrations/yt_dlp_plugin.py</code> <pre><code>def generate_urls(\n    self,\n    url: str,\n    extra_info_extract_opts: dict = None,\n    get_subs: bool = False,\n) -&gt; Iterable[tuple[str, str, str | None]]:\n    \"\"\"Generate URLs and download subtitles for a given video URL.\n\n    Args:\n        url (str): The URL of the video to download subtitles for.\n        extra_info_extract_opts (dict, optional): Additional options for extracting video information.\n\n    Yields:\n        tuple: A tuple containing the video title, video format URL, and downloaded subtitles.\n    \"\"\"\n    if extra_info_extract_opts is None:\n        extra_info_extract_opts = {}\n    if get_subs:\n        extra_info_extract_opts |= self.get_subtitles_opts()\n    extr_args = {\"ie_key\": self.ie_key} if \"ytsearch\" not in url else {}\n\n    def preproc_entry(info):\n        req_format = info[\"requested_formats\"][0]\n        subs = None\n        if get_subs and \"requested_subtitles\" in info:\n            subs = download_sub(\n                list(info[\"requested_subtitles\"].values())[0][\"url\"]\n            )\n        return info[\"title\"], req_format[\"url\"], subs\n\n    with YoutubeDL(params=(self.ydl_opts | extra_info_extract_opts)) as ydl:\n        info = ydl.extract_info(url, download=False, **extr_args)\n        if \"entries\" not in info:\n            yield preproc_entry(info)\n        else:\n            for entry in info.get(\"entries\", []):\n                if not entry:\n                    continue\n                yield preproc_entry(entry)\n</code></pre>"},{"location":"reference/video_sampler/integrations/yt_dlp_plugin/#video_sampler.integrations.yt_dlp_plugin.best_video_best_audio","title":"<code>best_video_best_audio(ctx)</code>","text":"<p>Taken from the yt-dlp documentation as-is</p> Source code in <code>video_sampler/integrations/yt_dlp_plugin.py</code> <pre><code>def best_video_best_audio(ctx):\n    \"\"\"Taken from the yt-dlp documentation as-is\"\"\"\n    \"\"\"Select the best video and the best audio that won't result in an mkv.\n    NOTE: This is just an example and does not handle all cases\"\"\"\n\n    # formats are already sorted worst to best\n    formats = ctx.get(\"formats\")[::-1]\n\n    # acodec='none' means there is no audio\n    best_video = next(\n        f for f in formats if f[\"vcodec\"] != \"none\" and f[\"acodec\"] == \"none\"\n    )\n\n    # find compatible audio extension\n    audio_ext = {\"mp4\": \"m4a\", \"webm\": \"webm\"}[best_video[\"ext\"]]\n    # vcodec='none' means there is no video\n    best_audio = next(\n        f\n        for f in formats\n        if (f[\"acodec\"] != \"none\" and f[\"vcodec\"] == \"none\" and f[\"ext\"] == audio_ext)\n    )\n\n    # These are the minimum required fields for a merged format\n    yield {\n        \"format_id\": f'{best_video[\"format_id\"]}+{best_audio[\"format_id\"]}',\n        \"ext\": best_video[\"ext\"],\n        \"requested_formats\": [best_video, best_audio],\n        # Must be + separated list of protocols\n        \"protocol\": f'{best_video[\"protocol\"]}+{best_audio[\"protocol\"]}',\n    }\n</code></pre>"},{"location":"reference/video_sampler/integrations/yt_dlp_plugin/#video_sampler.integrations.yt_dlp_plugin.best_video_only","title":"<code>best_video_only(ctx)</code>","text":"<p>Just best video -- save bandwidth</p> Source code in <code>video_sampler/integrations/yt_dlp_plugin.py</code> <pre><code>def best_video_only(ctx):\n    \"\"\"Just best video -- save bandwidth\"\"\"\n    # formats are already sorted worst to best\n    formats = ctx.get(\"formats\")[::-1]\n\n    # acodec='none' means there is no audio\n    best_video = next(f for f in formats if f[\"vcodec\"] != \"none\")\n    # These are the minimum required fields for a merged format\n    yield {\n        \"format_id\": f'{best_video[\"format_id\"]}',\n        \"ext\": best_video[\"ext\"],\n        \"requested_formats\": [best_video],\n        # Must be + separated list of protocols\n        \"protocol\": f'{best_video[\"protocol\"]}',\n    }\n</code></pre>"},{"location":"reference/video_sampler/integrations/yt_dlp_plugin/#video_sampler.integrations.yt_dlp_plugin.no_shorts","title":"<code>no_shorts(info, *, incomplete)</code>","text":"<p>Filter out short videos</p> Source code in <code>video_sampler/integrations/yt_dlp_plugin.py</code> <pre><code>def no_shorts(info, *, incomplete):\n    \"\"\"Filter out short videos\"\"\"\n    if url := info.get(\"url\", \"\"):\n        if \"/shorts\" in url:\n            return \"This is a short video\"\n</code></pre>"},{"location":"reference/video_sampler/language/keyword_capture/","title":"Language","text":""},{"location":"reference/video_sampler/language/keyword_capture/#video_sampler.language.keyword_capture.KeywordExtractor","title":"<code>KeywordExtractor</code>","text":"<p>Extracts keywords from subtitles using spaCy.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>list[str]</code> <p>List of keywords to extract.</p> required <p>Attributes:</p> Name Type Description <code>keywords</code> <code>list[str]</code> <p>List of keywords to extract.</p> <code>nlp</code> <p>spaCy language model for text processing.</p> <code>lemmatized_keywords</code> <code>set[str]</code> <p>Set of lemmatized keywords.</p> <p>Methods:</p> Name Description <code>generate_segments</code> <p>Captures keyword segments from a list of subtitles.</p> Source code in <code>video_sampler/language/keyword_capture.py</code> <pre><code>class KeywordExtractor:\n    \"\"\"\n    Extracts keywords from subtitles using spaCy.\n\n    Args:\n        keywords (list[str]): List of keywords to extract.\n\n    Attributes:\n        keywords (list[str]): List of keywords to extract.\n        nlp: spaCy language model for text processing.\n        lemmatized_keywords (set[str]): Set of lemmatized keywords.\n\n    Methods:\n        generate_segments: Captures keyword segments from a list of subtitles.\n\n    \"\"\"\n\n    def __init__(self, keywords: list[str]) -&gt; None:\n        try:\n            import spacy\n        except ImportError as e:\n            raise ImportError(\n                \"To use this feature install spacy by 'pip install spacy'\"\n            ) from e\n\n        self.keywords = keywords\n        self.nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"textcat\"])\n        self.lemmatized_keywords = {\n            tok.lemma_ for tok in self.nlp(\" \".join(self.keywords))\n        }\n        console.print(\n            f\"Keyword capture initialised with: {keywords}\",\n            style=f\"bold {Color.magenta.value}\",\n        )\n\n    def generate_segments(\n        self, subtitle_list: list[tuple[tuple[int, int], str]]\n    ) -&gt; Iterable[subtitle_line]:\n        \"\"\"\n        Captures keyword segments from a list of subtitles.\n\n        Args:\n            subtitle_list (list[tuple[tuple[int, int], str]]): List of subtitles in the format\n                (start_time, end_time, content).\n\n        Yields:\n            subtitle_line: A named tuple representing a keyword segment in the format\n                (start_time, end_time, lemma, content).\n\n        \"\"\"\n        for (start_time, end_time), content in subtitle_list:\n            doc = self.nlp(content.lower())\n            for lemma in doc:\n                if lemma.lemma_ in self.lemmatized_keywords:\n                    console.print(\n                        f\"Keyword {lemma.lemma_}: {start_time} - {end_time}\",\n                        style=f\"bold {Color.green.value}\",\n                    )\n                    yield subtitle_line(start_time, end_time, lemma, content)\n                    break\n</code></pre>"},{"location":"reference/video_sampler/language/keyword_capture/#video_sampler.language.keyword_capture.KeywordExtractor.generate_segments","title":"<code>generate_segments(subtitle_list)</code>","text":"<p>Captures keyword segments from a list of subtitles.</p> <p>Parameters:</p> Name Type Description Default <code>subtitle_list</code> <code>list[tuple[tuple[int, int], str]]</code> <p>List of subtitles in the format (start_time, end_time, content).</p> required <p>Yields:</p> Name Type Description <code>subtitle_line</code> <code>Iterable[subtitle_line]</code> <p>A named tuple representing a keyword segment in the format (start_time, end_time, lemma, content).</p> Source code in <code>video_sampler/language/keyword_capture.py</code> <pre><code>def generate_segments(\n    self, subtitle_list: list[tuple[tuple[int, int], str]]\n) -&gt; Iterable[subtitle_line]:\n    \"\"\"\n    Captures keyword segments from a list of subtitles.\n\n    Args:\n        subtitle_list (list[tuple[tuple[int, int], str]]): List of subtitles in the format\n            (start_time, end_time, content).\n\n    Yields:\n        subtitle_line: A named tuple representing a keyword segment in the format\n            (start_time, end_time, lemma, content).\n\n    \"\"\"\n    for (start_time, end_time), content in subtitle_list:\n        doc = self.nlp(content.lower())\n        for lemma in doc:\n            if lemma.lemma_ in self.lemmatized_keywords:\n                console.print(\n                    f\"Keyword {lemma.lemma_}: {start_time} - {end_time}\",\n                    style=f\"bold {Color.green.value}\",\n                )\n                yield subtitle_line(start_time, end_time, lemma, content)\n                break\n</code></pre>"},{"location":"reference/video_sampler/language/keyword_capture/#video_sampler.language.keyword_capture.download_sub","title":"<code>download_sub(sub_url, max_retries=2)</code>","text":"<p>Download a VTT subtitle file to a string with retry mechanism.</p> Source code in <code>video_sampler/language/keyword_capture.py</code> <pre><code>def download_sub(sub_url: str, max_retries: int = 2):\n    \"\"\"Download a VTT subtitle file to a string with retry mechanism.\"\"\"\n    for _ in range(max_retries):\n        try:\n            response = requests.get(url=sub_url)\n            response.raise_for_status()\n            return parse_srt_subtitle(response.text)\n        except RequestException as e:\n            console.print(f\"Download failed: {str(e)}\", style=f\"bold {Color.red.value}\")\n    return None\n</code></pre>"},{"location":"reference/video_sampler/language/keyword_capture/#video_sampler.language.keyword_capture.parse_srt_subtitle","title":"<code>parse_srt_subtitle(srt_content)</code>","text":"<p>Parse a SRT subtitle file to a list of subtitle segments.</p> Source code in <code>video_sampler/language/keyword_capture.py</code> <pre><code>def parse_srt_subtitle(srt_content: str) -&gt; list[tuple[tuple[int, int], str]]:\n    \"\"\"Parse a SRT subtitle file to a list of subtitle segments.\"\"\"\n    try:\n        import pysrt\n    except ImportError as e:\n        raise ImportError(\n            \"To use this feature install pysrt by 'pip install pysrt'\"\n        ) from e\n\n    subtitle_list = []\n    if not srt_content:\n        return subtitle_list\n    subs = pysrt.from_string(srt_content)\n    for sub in subs:\n        time = (sub.start.ordinal, sub.end.ordinal)\n        content = sub.text\n        subtitle_list.append((time, content))\n    return subtitle_list\n</code></pre>"},{"location":"reference/video_sampler/samplers/base_sampler/","title":"Samplers","text":""},{"location":"reference/video_sampler/samplers/base_sampler/#video_sampler.samplers.base_sampler.BaseSampler","title":"<code>BaseSampler</code>","text":"Source code in <code>video_sampler/samplers/base_sampler.py</code> <pre><code>class BaseSampler:\n    def __init__(self, cfg: SamplerConfig):\n        self.cfg: SamplerConfig = deepcopy(cfg)\n        self.frame_buffer: FrameBuffer = create_buffer(self.cfg.buffer_config)\n        self.gate: BlurGate | ClipGate | PassGate = create_gate(self.cfg.gate_config)\n        self.stats = Counter()\n\n    def sample(self, _: str) -&gt; Iterable[list[FrameObject]]:\n        raise NotImplementedError(\"sample method must be implemented\")\n\n    def write_queue(self, _: str, q: Queue, subs: str = None):\n        raise NotImplementedError(\"write_queue method must be implemented\")\n\n    def init_sampler(self):\n        self.stats.clear()\n        self.frame_buffer.clear()\n\n    def flush_buffer(self) -&gt; Iterable[list[FrameObject]]:\n        \"\"\"Flushes the frame buffer and yields gated frames\"\"\"\n        for res in self.frame_buffer.final_flush():\n            if res:\n                self.stats[\"produced\"] += 1\n                gated_obj: GatedObject = self.gate(*res)\n                self.stats[\"gated\"] += gated_obj.N\n                if gated_obj.frames:\n                    yield gated_obj.frames\n        gated_obj: GatedObject = self.gate.flush()\n        self.stats[\"gated\"] += gated_obj.N\n        if gated_obj.frames:\n            yield gated_obj.frames\n        yield PROCESSING_DONE_ITERABLE\n\n    def process_frame(\n        self, frame_indx: int, frame: Image, ftime: float\n    ) -&gt; Iterable[list[FrameObject]]:\n        if self.cfg.debug:\n            buf = self.frame_buffer.get_buffer_state()\n            console.print(\n                f\"Frame {frame_indx}\\ttime: {ftime}\",\n                f\"\\t Buffer ({len(buf)}): {buf}\",\n                style=f\"bold {Color.green.value}\",\n            )\n        frame_meta = {\"frame_time\": ftime, \"frame_indx\": frame_indx}\n        self.stats[\"decoded\"] += 1\n        if res := self.frame_buffer.add(\n            frame,\n            metadata=frame_meta,\n        ):\n            gated_obj: GatedObject = self.gate(*res)\n            self.stats[\"produced\"] += 1\n            self.stats[\"gated\"] += gated_obj.N\n            if gated_obj.frames:\n                yield gated_obj.frames\n</code></pre>"},{"location":"reference/video_sampler/samplers/base_sampler/#video_sampler.samplers.base_sampler.BaseSampler.flush_buffer","title":"<code>flush_buffer()</code>","text":"<p>Flushes the frame buffer and yields gated frames</p> Source code in <code>video_sampler/samplers/base_sampler.py</code> <pre><code>def flush_buffer(self) -&gt; Iterable[list[FrameObject]]:\n    \"\"\"Flushes the frame buffer and yields gated frames\"\"\"\n    for res in self.frame_buffer.final_flush():\n        if res:\n            self.stats[\"produced\"] += 1\n            gated_obj: GatedObject = self.gate(*res)\n            self.stats[\"gated\"] += gated_obj.N\n            if gated_obj.frames:\n                yield gated_obj.frames\n    gated_obj: GatedObject = self.gate.flush()\n    self.stats[\"gated\"] += gated_obj.N\n    if gated_obj.frames:\n        yield gated_obj.frames\n    yield PROCESSING_DONE_ITERABLE\n</code></pre>"},{"location":"reference/video_sampler/samplers/image_sampler/","title":"Image sampler","text":""},{"location":"reference/video_sampler/samplers/image_sampler/#video_sampler.samplers.image_sampler.ImageSampler","title":"<code>ImageSampler</code>","text":"<p>             Bases: <code>BaseSampler</code></p> <p>Image sampler -- sample frames from a folder of images</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>ImageSamplerConfig</code> <p>Image sampler config</p> required <p>Methods:</p> Name Description <code>sample</code> <p>str) -&gt; Iterable[list[FrameObject]]: Sample frames from image folder</p> <code>write_queue</code> <p>str, q: Queue, _: str = None): Write frames to queue</p> Source code in <code>video_sampler/samplers/image_sampler.py</code> <pre><code>class ImageSampler(BaseSampler):\n    \"\"\"\n    Image sampler -- sample frames from a folder of images\n\n    Args:\n        cfg (ImageSamplerConfig): Image sampler config\n\n    Methods:\n        sample(image_folder: str) -&gt; Iterable[list[FrameObject]]: Sample frames from image folder\n        write_queue(image_path: str, q: Queue, _: str = None): Write frames to queue\n    \"\"\"\n\n    def __init__(self, cfg: ImageSamplerConfig):\n        super().__init__(cfg)\n        self.rgx = None\n        if cfg.frame_time_regex:\n            console.print(\n                f\"Using frame time regex: {cfg.frame_time_regex}\", style=\"bold yellow\"\n            )\n            self.rgx = re.compile(cfg.frame_time_regex)\n\n    def extract_frame_time(self, image_path: str, default: str | None = None) -&gt; str:\n        \"\"\"\n        Extract frame time from image path\n        Args:\n            image_path (str): Path to image\n            default (str | None): Default frame time to return if no regex is set\n\n        Returns:\n            str: Frame time\n        \"\"\"\n        if self.rgx:\n            if match := self.rgx.search(image_path):\n                return float(match.group(1))\n            else:\n                console.print(\n                    f\"No frame time found in {image_path} with regex {self.rgx}\",\n                    style=\"bold red\",\n                )\n        if default is None:\n            raise ValueError(\n                f\"Frame time regex is not set, can't extract frame name from {image_path}\"\n            )\n        return default\n\n    def sample(self, image_folder: str) -&gt; Iterable[list[FrameObject]]:\n        \"\"\"\n        Sample frames from image folder\n        Args:\n            image_folder (str): Path to image folder or glob pattern\n\n        Returns:\n            Iterable[list[FrameObject]]: Iterable of frames\n        \"\"\"\n        self.init_sampler()\n        if \"*\" in image_folder:\n            image_paths = glob.iglob(image_folder)\n        else:\n            # iterable over all files in image_folder\n            image_paths = (\n                os.path.join(image_folder, f) for f in os.listdir(image_folder)\n            )\n        generator = sorted(\n            image_paths,\n            key=lambda x: self.extract_frame_time(os.path.basename(x), default=x),\n        )\n        for frame_indx, image_path in enumerate(generator):\n            frame = Image.open(image_path)\n            yield from self.process_frame(\n                frame_indx=frame_indx,\n                frame=frame,\n                ftime=self.extract_frame_time(os.path.basename(image_path), frame_indx),\n            )\n\n        yield from self.flush_buffer()\n\n    def write_queue(self, image_path: str, q: Queue, _: str = None):\n        \"\"\"\n        Write frames to queue\n        Args:\n            image_path (str): Path to image\n            q (Queue): Queue to write frames to\n        \"\"\"\n        try:\n            for item in self.sample(image_path):\n                q.put(item)\n        except Exception as e:\n            console.print(\n                f\"Error while processing {image_path}\",\n                f\"\\n\\t{e}\",\n                style=f\"bold {Color.red.value}\",\n            )\n            q.put(PROCESSING_DONE_ITERABLE)\n</code></pre>"},{"location":"reference/video_sampler/samplers/image_sampler/#video_sampler.samplers.image_sampler.ImageSampler.extract_frame_time","title":"<code>extract_frame_time(image_path, default=None)</code>","text":"<p>Extract frame time from image path Args:     image_path (str): Path to image     default (str | None): Default frame time to return if no regex is set</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Frame time</p> Source code in <code>video_sampler/samplers/image_sampler.py</code> <pre><code>def extract_frame_time(self, image_path: str, default: str | None = None) -&gt; str:\n    \"\"\"\n    Extract frame time from image path\n    Args:\n        image_path (str): Path to image\n        default (str | None): Default frame time to return if no regex is set\n\n    Returns:\n        str: Frame time\n    \"\"\"\n    if self.rgx:\n        if match := self.rgx.search(image_path):\n            return float(match.group(1))\n        else:\n            console.print(\n                f\"No frame time found in {image_path} with regex {self.rgx}\",\n                style=\"bold red\",\n            )\n    if default is None:\n        raise ValueError(\n            f\"Frame time regex is not set, can't extract frame name from {image_path}\"\n        )\n    return default\n</code></pre>"},{"location":"reference/video_sampler/samplers/image_sampler/#video_sampler.samplers.image_sampler.ImageSampler.sample","title":"<code>sample(image_folder)</code>","text":"<p>Sample frames from image folder Args:     image_folder (str): Path to image folder or glob pattern</p> <p>Returns:</p> Type Description <code>Iterable[list[FrameObject]]</code> <p>Iterable[list[FrameObject]]: Iterable of frames</p> Source code in <code>video_sampler/samplers/image_sampler.py</code> <pre><code>def sample(self, image_folder: str) -&gt; Iterable[list[FrameObject]]:\n    \"\"\"\n    Sample frames from image folder\n    Args:\n        image_folder (str): Path to image folder or glob pattern\n\n    Returns:\n        Iterable[list[FrameObject]]: Iterable of frames\n    \"\"\"\n    self.init_sampler()\n    if \"*\" in image_folder:\n        image_paths = glob.iglob(image_folder)\n    else:\n        # iterable over all files in image_folder\n        image_paths = (\n            os.path.join(image_folder, f) for f in os.listdir(image_folder)\n        )\n    generator = sorted(\n        image_paths,\n        key=lambda x: self.extract_frame_time(os.path.basename(x), default=x),\n    )\n    for frame_indx, image_path in enumerate(generator):\n        frame = Image.open(image_path)\n        yield from self.process_frame(\n            frame_indx=frame_indx,\n            frame=frame,\n            ftime=self.extract_frame_time(os.path.basename(image_path), frame_indx),\n        )\n\n    yield from self.flush_buffer()\n</code></pre>"},{"location":"reference/video_sampler/samplers/image_sampler/#video_sampler.samplers.image_sampler.ImageSampler.write_queue","title":"<code>write_queue(image_path, q, _=None)</code>","text":"<p>Write frames to queue Args:     image_path (str): Path to image     q (Queue): Queue to write frames to</p> Source code in <code>video_sampler/samplers/image_sampler.py</code> <pre><code>def write_queue(self, image_path: str, q: Queue, _: str = None):\n    \"\"\"\n    Write frames to queue\n    Args:\n        image_path (str): Path to image\n        q (Queue): Queue to write frames to\n    \"\"\"\n    try:\n        for item in self.sample(image_path):\n            q.put(item)\n    except Exception as e:\n        console.print(\n            f\"Error while processing {image_path}\",\n            f\"\\n\\t{e}\",\n            style=f\"bold {Color.red.value}\",\n        )\n        q.put(PROCESSING_DONE_ITERABLE)\n</code></pre>"},{"location":"reference/video_sampler/samplers/video_sampler/","title":"Video sampler","text":""},{"location":"reference/video_sampler/samplers/video_sampler/#video_sampler.samplers.video_sampler.SegmentSampler","title":"<code>SegmentSampler</code>","text":"<p>             Bases: <code>VideoSampler</code></p> <p>A class for sampling video frames based on subtitle segments.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>SamplerConfig</code> <p>The configuration for the video sampler.</p> required <code>segment_generator</code> <code>Iterable[subtitle_line]</code> <p>An iterable of subtitle segments.</p> required <p>Methods:</p> Name Description <code>sample</code> <p>Generates sample frames from a video.</p> <code>write_queue</code> <p>Writes sampled frames to a queue.</p> Source code in <code>video_sampler/samplers/video_sampler.py</code> <pre><code>class SegmentSampler(VideoSampler):\n    \"\"\"\n    A class for sampling video frames based on subtitle segments.\n\n    Args:\n        cfg (SamplerConfig): The configuration for the video sampler.\n        segment_generator (Iterable[subtitle_line]): An iterable of subtitle segments.\n\n    Methods:\n        sample(video_path) -&gt; Iterable[list[FrameObject]]:\n            Generates sample frames from a video.\n        write_queue(video_path, q):\n            Writes sampled frames to a queue.\n    \"\"\"\n\n    def __init__(self, cfg: SamplerConfig) -&gt; None:\n        super().__init__(cfg)\n        self.extractor = create_extractor(cfg.extractor_config)\n\n    def sample(self, video_path: str, subs: str = None) -&gt; Iterable[list[FrameObject]]:\n        \"\"\"Generate sample frames from a video.\n\n        Args:\n            video_path (str): The path to the video file.\n            subs (str): Subtitles for the video file.\n\n        Yields:\n            Iterable[list[FrameObject]]: A generator that yields a list of FrameObjects representing sampled frames.\n        \"\"\"\n        segment_generator: Iterable[subtitle_line] = self.extractor.generate_segments(\n            subs\n        )\n        self.init_sampler()\n        next_segment = next(segment_generator)\n        segment_boundary_end_sec = next_segment.end_time / 1000\n        segment_boundary_start_sec = next_segment.start_time / 1000\n        absolute_stop = False\n        with av.open(video_path) as container:\n            stream = container.streams.video[0]\n            if self.cfg.keyframes_only:\n                stream.codec_context.skip_frame = \"NONKEY\"\n            prev_time = -10\n            for frame_indx, frame in enumerate(container.decode(stream)):\n                if frame is None:\n                    continue\n                try:\n                    ftime = frame.time\n                except AttributeError:\n                    continue\n                reiters = 0\n                # find the next segment that starts after the current frame\n                while ftime &gt; segment_boundary_end_sec:\n                    console.print(\n                        f\"Seeking to next segment: {segment_boundary_end_sec}/{ftime}\",\n                        style=f\"bold {Color.yellow.value}\",\n                    )\n                    try:\n                        next_segment = next(segment_generator)\n                        reiters += 1\n                        segment_boundary_end_sec = next_segment.end_time / 1000\n                        segment_boundary_start_sec = next_segment.start_time / 1000\n                    except StopIteration:\n                        absolute_stop = True\n                        break\n                if reiters &gt; 0:\n                    console.print(\n                        f\"Skipped {reiters} segments!\",\n                        style=f\"bold {Color.red.value}\",\n                    )\n                if absolute_stop:\n                    break\n                # we haven't found the next segment yet\n                # the other condition, is where we are after the segment\n                # but this is handled by the while loop above\n                if ftime &lt;= segment_boundary_start_sec:\n                    continue\n\n                self.stats[\"total\"] += 1\n                time_diff = ftime - prev_time\n                if time_diff &lt; self.cfg.min_frame_interval_sec:\n                    continue\n                prev_time = ftime\n                frame_pil = frame.to_image()\n                yield from self.process_frame(frame_indx, frame_pil, ftime)\n        # flush buffer\n        yield from self.flush_buffer()\n\n    def write_queue(self, video_path: str, q: Queue, subs: str = None):\n        super().write_queue(video_path, q, subs=subs)\n</code></pre>"},{"location":"reference/video_sampler/samplers/video_sampler/#video_sampler.samplers.video_sampler.SegmentSampler.sample","title":"<code>sample(video_path, subs=None)</code>","text":"<p>Generate sample frames from a video.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>The path to the video file.</p> required <code>subs</code> <code>str</code> <p>Subtitles for the video file.</p> <code>None</code> <p>Yields:</p> Type Description <code>Iterable[list[FrameObject]]</code> <p>Iterable[list[FrameObject]]: A generator that yields a list of FrameObjects representing sampled frames.</p> Source code in <code>video_sampler/samplers/video_sampler.py</code> <pre><code>def sample(self, video_path: str, subs: str = None) -&gt; Iterable[list[FrameObject]]:\n    \"\"\"Generate sample frames from a video.\n\n    Args:\n        video_path (str): The path to the video file.\n        subs (str): Subtitles for the video file.\n\n    Yields:\n        Iterable[list[FrameObject]]: A generator that yields a list of FrameObjects representing sampled frames.\n    \"\"\"\n    segment_generator: Iterable[subtitle_line] = self.extractor.generate_segments(\n        subs\n    )\n    self.init_sampler()\n    next_segment = next(segment_generator)\n    segment_boundary_end_sec = next_segment.end_time / 1000\n    segment_boundary_start_sec = next_segment.start_time / 1000\n    absolute_stop = False\n    with av.open(video_path) as container:\n        stream = container.streams.video[0]\n        if self.cfg.keyframes_only:\n            stream.codec_context.skip_frame = \"NONKEY\"\n        prev_time = -10\n        for frame_indx, frame in enumerate(container.decode(stream)):\n            if frame is None:\n                continue\n            try:\n                ftime = frame.time\n            except AttributeError:\n                continue\n            reiters = 0\n            # find the next segment that starts after the current frame\n            while ftime &gt; segment_boundary_end_sec:\n                console.print(\n                    f\"Seeking to next segment: {segment_boundary_end_sec}/{ftime}\",\n                    style=f\"bold {Color.yellow.value}\",\n                )\n                try:\n                    next_segment = next(segment_generator)\n                    reiters += 1\n                    segment_boundary_end_sec = next_segment.end_time / 1000\n                    segment_boundary_start_sec = next_segment.start_time / 1000\n                except StopIteration:\n                    absolute_stop = True\n                    break\n            if reiters &gt; 0:\n                console.print(\n                    f\"Skipped {reiters} segments!\",\n                    style=f\"bold {Color.red.value}\",\n                )\n            if absolute_stop:\n                break\n            # we haven't found the next segment yet\n            # the other condition, is where we are after the segment\n            # but this is handled by the while loop above\n            if ftime &lt;= segment_boundary_start_sec:\n                continue\n\n            self.stats[\"total\"] += 1\n            time_diff = ftime - prev_time\n            if time_diff &lt; self.cfg.min_frame_interval_sec:\n                continue\n            prev_time = ftime\n            frame_pil = frame.to_image()\n            yield from self.process_frame(frame_indx, frame_pil, ftime)\n    # flush buffer\n    yield from self.flush_buffer()\n</code></pre>"},{"location":"reference/video_sampler/samplers/video_sampler/#video_sampler.samplers.video_sampler.VideoSampler","title":"<code>VideoSampler</code>","text":"<p>             Bases: <code>BaseSampler</code></p> <p>The fundamental class for sampling video frames.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>SamplerConfig</code> <p>The configuration for the video sampler.</p> required <p>Attributes:</p> Name Type Description <code>cfg</code> <code>SamplerConfig</code> <p>The configuration for the video sampler.</p> <code>frame_buffer</code> <code>FrameBuffer</code> <p>The frame buffer used for sampling frames.</p> <code>gate</code> <code>Gate</code> <p>The gate used for filtering frames.</p> <code>stats</code> <code>Counter</code> <p>A counter for tracking statistics.</p> <p>Methods:</p> Name Description <code>sample</code> <p>Generates sample frames from a video.</p> <code>write_queue</code> <p>Writes sampled frames to a queue.</p> Source code in <code>video_sampler/samplers/video_sampler.py</code> <pre><code>class VideoSampler(BaseSampler):\n    \"\"\"\n    The fundamental class for sampling video frames.\n\n    Args:\n        cfg (SamplerConfig): The configuration for the video sampler.\n\n    Attributes:\n        cfg (SamplerConfig): The configuration for the video sampler.\n        frame_buffer (FrameBuffer): The frame buffer used for sampling frames.\n        gate (Gate): The gate used for filtering frames.\n        stats (Counter): A counter for tracking statistics.\n\n    Methods:\n        sample(video_path) -&gt; Iterable[list[FrameObject]]:\n            Generates sample frames from a video.\n        write_queue(video_path, q):\n            Writes sampled frames to a queue.\n\n    \"\"\"\n\n    def __init__(self, cfg: SamplerConfig) -&gt; None:\n        super().__init__(cfg)\n\n    def sample(self, video_path: str, subs: str = None) -&gt; Iterable[list[FrameObject]]:\n        \"\"\"Generate sample frames from a video.\n\n        Args:\n            video_path (str): The path to the video file.\n            subs (str): Unused in video sampler\n\n        Yields:\n            Iterable[list[FrameObject]]: A generator that yields a list\n                    of FrameObjects representing sampled frames.\n        \"\"\"\n        self.init_sampler()\n        with av.open(video_path) as container:\n            stream = container.streams.video[0]\n            if self.cfg.keyframes_only:\n                stream.codec_context.skip_frame = \"NONKEY\"\n            prev_time = -10\n            if self.cfg.start_time_s &gt; 0 and self.cfg.precise_seek is False:\n                \"\"\"\n                This is an inaccurate seek, so you may get frames before\n                the start time or much later.\n                \"\"\"\n                frame_container_pts = (\n                    int(self.cfg.start_time_s / float(stream.time_base))\n                    + stream.start_time\n                )\n                console.print(\n                    f\"Seeking to {frame_container_pts} pts based of {stream.time_base} \"\n                    f\"with inbuilt video offset {stream.start_time}\",\n                    style=f\"bold {Color.yellow.value}\",\n                )\n                try:\n                    container.seek(\n                        frame_container_pts,\n                        backward=True,\n                        any_frame=False,\n                        stream=stream,\n                    )\n                except av.AVError as e:\n                    console.print(\n                        f\"Error seeking to {frame_container_pts} pts. Will default to precise seek.\",\n                        f\"\\n\\t{e}\",\n                        style=f\"bold {Color.red.value}\",\n                    )\n            avg_fps = float(stream.average_rate)\n            for frame in container.decode(stream):\n                if frame is None or frame.is_corrupt:\n                    continue\n                try:\n                    ftime = frame.time\n                except AttributeError:\n                    continue\n                if self.cfg.start_time_s &gt; 0 and ftime &lt; self.cfg.start_time_s:\n                    continue\n\n                if self.cfg.end_time_s is not None and ftime &gt; self.cfg.end_time_s:\n                    break\n                frame_index = int(ftime * avg_fps)\n                # skip frames if keyframes_only is True\n                time_diff = ftime - prev_time\n                self.stats[\"total\"] += 1\n                if time_diff &lt; self.cfg.min_frame_interval_sec:\n                    continue\n                prev_time = ftime\n                frame_pil = frame.to_image()\n                yield from self.process_frame(frame_index, frame_pil, ftime)\n        # flush buffer\n        yield from self.flush_buffer()\n\n    def write_queue(self, video_path: str, q: Queue, subs: str = None) -&gt; None:\n        try:\n            item: tuple[FrameObject, int]\n            for item in self.sample(video_path=video_path, subs=subs):\n                q.put(item)\n        except (av.IsADirectoryError, av.InvalidDataError) as e:\n            console.print(\n                f\"Error while processing {video_path}\",\n                f\"\\n\\t{e}\",\n                style=f\"bold {Color.red.value}\",\n            )\n            q.put(PROCESSING_DONE_ITERABLE)\n</code></pre>"},{"location":"reference/video_sampler/samplers/video_sampler/#video_sampler.samplers.video_sampler.VideoSampler.sample","title":"<code>sample(video_path, subs=None)</code>","text":"<p>Generate sample frames from a video.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>The path to the video file.</p> required <code>subs</code> <code>str</code> <p>Unused in video sampler</p> <code>None</code> <p>Yields:</p> Type Description <code>Iterable[list[FrameObject]]</code> <p>Iterable[list[FrameObject]]: A generator that yields a list     of FrameObjects representing sampled frames.</p> Source code in <code>video_sampler/samplers/video_sampler.py</code> <pre><code>def sample(self, video_path: str, subs: str = None) -&gt; Iterable[list[FrameObject]]:\n    \"\"\"Generate sample frames from a video.\n\n    Args:\n        video_path (str): The path to the video file.\n        subs (str): Unused in video sampler\n\n    Yields:\n        Iterable[list[FrameObject]]: A generator that yields a list\n                of FrameObjects representing sampled frames.\n    \"\"\"\n    self.init_sampler()\n    with av.open(video_path) as container:\n        stream = container.streams.video[0]\n        if self.cfg.keyframes_only:\n            stream.codec_context.skip_frame = \"NONKEY\"\n        prev_time = -10\n        if self.cfg.start_time_s &gt; 0 and self.cfg.precise_seek is False:\n            \"\"\"\n            This is an inaccurate seek, so you may get frames before\n            the start time or much later.\n            \"\"\"\n            frame_container_pts = (\n                int(self.cfg.start_time_s / float(stream.time_base))\n                + stream.start_time\n            )\n            console.print(\n                f\"Seeking to {frame_container_pts} pts based of {stream.time_base} \"\n                f\"with inbuilt video offset {stream.start_time}\",\n                style=f\"bold {Color.yellow.value}\",\n            )\n            try:\n                container.seek(\n                    frame_container_pts,\n                    backward=True,\n                    any_frame=False,\n                    stream=stream,\n                )\n            except av.AVError as e:\n                console.print(\n                    f\"Error seeking to {frame_container_pts} pts. Will default to precise seek.\",\n                    f\"\\n\\t{e}\",\n                    style=f\"bold {Color.red.value}\",\n                )\n        avg_fps = float(stream.average_rate)\n        for frame in container.decode(stream):\n            if frame is None or frame.is_corrupt:\n                continue\n            try:\n                ftime = frame.time\n            except AttributeError:\n                continue\n            if self.cfg.start_time_s &gt; 0 and ftime &lt; self.cfg.start_time_s:\n                continue\n\n            if self.cfg.end_time_s is not None and ftime &gt; self.cfg.end_time_s:\n                break\n            frame_index = int(ftime * avg_fps)\n            # skip frames if keyframes_only is True\n            time_diff = ftime - prev_time\n            self.stats[\"total\"] += 1\n            if time_diff &lt; self.cfg.min_frame_interval_sec:\n                continue\n            prev_time = ftime\n            frame_pil = frame.to_image()\n            yield from self.process_frame(frame_index, frame_pil, ftime)\n    # flush buffer\n    yield from self.flush_buffer()\n</code></pre>"},{"location":"reference/video_sampler/visualisation/clustering/","title":"Visualisation","text":""},{"location":"reference/video_sampler/visualisation/clustering/#video_sampler.visualisation.clustering.build_feature_model","title":"<code>build_feature_model(model_str)</code>","text":"<p>Build a feature extraction model.</p> <p>Parameters:</p> Name Type Description Default <code>model_str</code> <code>str</code> <p>Model name.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>Tuple of (model, extractor).</p> Source code in <code>video_sampler/visualisation/clustering.py</code> <pre><code>def build_feature_model(model_str: str):\n    \"\"\"Build a feature extraction model.\n\n    Args:\n        model_str (str): Model name.\n\n    Returns:\n        tuple: Tuple of (model, extractor).\n    \"\"\"\n    try:\n        from transformers import AutoFeatureExtractor, ResNetModel\n    except ImportError as e:\n        raise ImportError(\n            \"To use this feature install transformers by 'pip install transformers'\"\n        ) from e\n\n    extractor = AutoFeatureExtractor.from_pretrained(model_str)\n    model = ResNetModel.from_pretrained(model_str)\n    return model, extractor\n</code></pre>"},{"location":"reference/video_sampler/visualisation/clustering/#video_sampler.visualisation.clustering.cluster_features","title":"<code>cluster_features(features, max_clusters=50)</code>","text":"<p>Cluster features using t-SNE and KMeans</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>ndarray</code> <p>dict with keys \"embeds\" and \"paths\"</p> required <code>max_clusters</code> <code>int</code> <p>maximum number of clusters</p> <code>50</code> Retruns <p>tuple: of (X, cluster_labels)</p> Source code in <code>video_sampler/visualisation/clustering.py</code> <pre><code>def cluster_features(\n    features,\n    max_clusters=50,\n):\n    \"\"\"Cluster features using t-SNE and KMeans\n\n    Args:\n        features (np.ndarray): dict with keys \"embeds\" and \"paths\"\n        max_clusters (int): maximum number of clusters\n\n    Retruns:\n      tuple: of (X, cluster_labels)\n    \"\"\"\n    proj = TSNE(n_components=2, perplexity=35, metric=\"cosine\")\n    Xorg = np.asarray(features[\"embeds\"])\n    X = proj.fit_transform(Xorg)\n\n    # take about 10% of the frame as the number of clusters\n    n_clusters = min(int(0.1 * len(features[\"embeds\"])), max_clusters)\n    cluster_model = KMeans(n_clusters=n_clusters, random_state=0).fit(Xorg)\n    return X, cluster_model.labels_\n</code></pre>"},{"location":"reference/video_sampler/visualisation/clustering/#video_sampler.visualisation.clustering.extract_features","title":"<code>extract_features(model_str, image_folder, mkey='pixel_values', batch_size=8)</code>","text":"<p>Extract features from a folder of images.</p> <p>Parameters:</p> Name Type Description Default <code>model_str</code> <code>str</code> <p>Model name.</p> required <code>image_folder</code> <code>Path</code> <p>Folder with images.</p> required <code>mkey</code> <code>str</code> <p>Key for the pixel values. Defaults to \"pixel_values\".</p> <code>'pixel_values'</code> <code>batch_size</code> <code>int</code> <p>Batch size. Defaults to 8.</p> <code>8</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary with keys \"embeds\" and \"paths\".</p> Source code in <code>video_sampler/visualisation/clustering.py</code> <pre><code>def extract_features(\n    model_str: str, image_folder: Path, mkey=\"pixel_values\", batch_size: int = 8\n):\n    \"\"\"Extract features from a folder of images.\n\n    Args:\n        model_str (str): Model name.\n        image_folder (Path): Folder with images.\n        mkey (str, optional): Key for the pixel values. Defaults to \"pixel_values\".\n        batch_size (int, optional): Batch size. Defaults to 8.\n\n    Returns:\n        dict: Dictionary with keys \"embeds\" and \"paths\".\n    \"\"\"\n\n    out_features = defaultdict(list)\n    model, extractor = build_feature_model(model_str)\n    with torch.no_grad():\n        all_files = list(image_folder.iterdir())\n        for batch in tqdm(\n            batched(all_files, batch_size), total=len(all_files) // batch_size\n        ):\n            # load images\n            batch_imgs = [Image.open(img_path).convert(\"RGB\") for img_path in batch]\n            # extract features\n            batch_imgs = extractor(batch_imgs, return_tensors=\"pt\")[mkey]\n            batch_features = model(batch_imgs).pooler_output.squeeze()\n            if len(batch) == 1:\n                batch_features = batch_features.expand(1, -1)\n            batch_features = torch.functional.F.normalize(batch_features, p=2, dim=1)\n            out_features[\"embeds\"].extend(batch_features)\n            out_features[\"paths\"].extend([img_path.name for img_path in batch])\n    return out_features\n</code></pre>"}]}